import os
#new_path = "/nfs/project/JayMok/power_experiments_xie/primitives"
#os.environ['LD_LIBRARY_PATH'] = os.pathsep.join([new_path, os.environ.get('LD_LIBRARY_PATH', '')])
#print(os.environ['LD_LIBRARY_PATH'])
IS_LINUX = 1
SBT = "/afs/ee.ust.hk/staff/ee/jaymok/.local/share/coursier/bin/sbt"

if(IS_LINUX):
	import ctypes
	ctypes.CDLL('/nfs/project/JayMok/power_experiments_xie/primitives/libstdc++.so.6')
	ctypes.cdll.LoadLibrary('/nfs/project/JayMok/power_experiments_xie/primitives/libstdc++.so.6')

#a hack

#A simple model, where each operation is a seperate module
#Spatial Architecture
from abc import ABC, abstractmethod
import pickle

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn.utils.validation")
#script generated by AI deepseek
import paddle.nn as nn

import os
import json

#Helpers
def log1p_inverse(x):
	return np.exp(x) - 1
		

def bit_count_helper():
	return """
	// 计算单个数字的1的数量
	int countBits(int num) {
	    int count = 0;
	    while (num) {
	        num &= (num - 1); // 清除最低位的1
	        count++;
	    }
	    return count;
	}
	
	// 对整个数组进行计算，并保存到新数组
	int* countBitsInArray(int* arr, int size) {
	    // 创建一个新数组，用于存储每个数字的1的数量
	    int* bitCounts = new int[size];
	
	    // 遍历原始数组，计算每个数字的1的数量
	    for (int i = 0; i < size; i++) {
	        bitCounts[i] = countBits(arr[i]);
	    }
	
	    return bitCounts; // 返回新数组
	}
	"""

def cpp_read_file_helper():
    return '''
	#include <iostream>
	#include <fstream>
	#include <cstdlib>  // Pour malloc et free

	// Fonction pour lire un fichier et stocker les valeurs dans un tableau
	int* lireFichierEtRemplirTableau(const char* filename, int*size, int count) {
		std::ifstream file(filename);
		if (!file.is_open()) {
			std::cerr << "Erreur : Impossible d'ouvrir le fichier." << std::endl;
			return nullptr;
		}

		int count2 = 0;
		std::string line;
		while (std::getline(file, line)) {
			count2++;
		}

		file.clear();  // Effacer les flags d'erreur
		file.seekg(0); // Retourner au début du fichier

		int* array = (int*)malloc(count * sizeof(int));
		if (array == nullptr) {
			std::cerr << "Erreur : chec de l'allocation mémoire." << std::endl;
			file.close();
			return nullptr;
		}

		// Initialiser le tableau avec des zéros
		for (int i = 0; i < count; i++) {
			array[i] = 0;
		}

		int index = 0;
		while (std::getline(file, line)) {
			array[index] = std::stoi(line); // Convertir la ligne en entier
			index++;
		}

		file.close();

		*size = count2;

		return array;
	}
	'''

class AIChip(ABC):
    """基类，定义了一些抽象方法。"""

    @abstractmethod
    def infer_cnn(self, input_data):
        """卷积神经网络推理。"""
        pass

    @abstractmethod
    def infer_fc(self, input_data):
        """全连接层推理。"""
        pass

    @abstractmethod
    def infer_avg_pool(self, input_data):
        """平均池化推理。"""
        pass

    @abstractmethod
    def infer_max_pool(self, input_data):
        """最大池化推理。"""
        pass

    @abstractmethod
    def infer_act(self, input_data):
        """激活函数推理。"""
        pass

    @abstractmethod
    def infer_element_add(self, input_data1, input_data2):
        """元素相加推理。"""
        pass

    @abstractmethod
    def infer_softmax(self, input_data):
        """Softmax 推理。"""
        pass

  #   def infer_cnn(self, input_data):
  #       """
  #       实现卷积操作。
  #       :param input_data: 输入数据，形状为 (B, I, H, W)
  #       :return: 卷积结果，形状为 (B, N, H', W')
  #       """
  #       # 假设卷积核大小为 3x3
  #       kernel_size = 3
  #       H, W = input_data.shape[2], input_data.shape[3]
  #       output_H = H - kernel_size + 1
  #       output_W = W - kernel_size + 1

  #       # 随机初始化卷积核
  #       kernel = np.random.randn(self.N, self.I, kernel_size, kernel_size)

  #       # 执行卷积操作
  #       output = np.zeros((self.B, self.N, output_H, output_W))
  #       for b in range(self.B):
  #           for n in range(self.N):
  #               for i in range(self.I):
  #                   output[b, n] += np.convolve(input_data[b, i].flatten(), kernel[n, i].flatten(), mode='valid').reshape(output_H, output_W)

  #       return output



  #   def infer_fc_old(self, input_data, weights):
  #       """
  #       实现全连接层操作。
  #       :param input_data: 输入数据，形状为 (B, I)
  #       :return: 全连接结果，形状为 (B, N)
  #       """
  #       input_data = input_data[0]#if is tuple
		
  #       print(input_data)
  #       print(weights)
  #       # 随机初始化权重
  #       # weights = np.random.randn(self.I, self.N)
  #       IN =  weights.shape[0]
  #       OUT = weights.shape[1]
  #       BAT = input_data.shape[0]
  #       assert(input_data.shape[1] == weights.shape[0])
  #       # 执行矩阵乘法
              
  #       #choose a loop-order here
  #       DRAM = []
  #       DRAM_multicast = []
  #       L2_buffer = []
  #       L2_multicast_act = []
  #       L2_multicast_wei = []
  #       L1_act_buffer = []
  #       L1_wei_buffer = []
  #       multicast_act = []
  #       multicast_wei = []
  #       multipliers = []
  #       adderN = []
  #       accum = []
  #       L1_accum_buffer = []
        
  #       for m in range(self.DRAM_LEN // self.L2_LEN):
  #           DRAM.append(open(f"DRAM_{m}.txt", "w"))
        
  #       for m in range( max(self.L2_LEN // self.L1_ACT_LEN, self.L2_LEN // self.L1_ACT_LEN) ):
  #           L2_buffer.append(open(f"L2_{m}.txt", "w"))
        
  #       #split the buffer into 16x...
  #       for m in range(max(( self.FC_TI*self.FC_TB)//self.L1_ACT_LEN  , 1)):
  #           L1_act_buffer.append(open(f"L1_act_{m}.txt","w"))
        
  #       for m in range(max(( self.FC_TN*self.FC_TB)//self.L1_WEI_LEN , 1)):
  #           L1_wei_buffer.append(open(f"L1_wei_{m}.txt","w"))
        
  #       for m in range(self.FC_TN):
  #           multicast_act.append(open(f"multicast_act_{m}.txt","w"))
        
  #       for m in range(self.FC_TB):
  #           multicast_wei.append(open(f"multicast_wei_{m}.txt","w"))
        
  #       for m in range(self.FC_TN*self.FC_TB*self.FC_TI):
  #       	multipliers.append(open(f"multipliers2_{m}.txt","w"))
        	
  #       for m in range(self.FC_TN*self.FC_TB):
  #       	adderN.append(open(f"adderN_{m}.txt","w"))
        	
  #       for m in range(self.FC_TN*self.FC_TB):
  #       	accum.append(open(f"accum_{m}.txt","w"))
        
  #       #################################
  #       #Change-able
  #       for i in range(0,IN,self.FC_TI):
  #       	for n in range(0,OUT, self.FC_TN):
  #       		for b in range(0,BAT, self.FC_TB):				
  #       			# for bb in range(b, min(b+self.FC_TB, BAT)):
  #       			# 	for ii in range(i, min(i+self.FC_TI, IN)):
  #       			# 		L1_act_buffer[].write()
        					
  #       			# for nn in range(n, min(n+self.FC_TN, OUT)):
  #       			# 	for ii in range(i, min(i+self.FC_TI, IN)):
  #       			# 		L1_wei_buffer[].write()
        					
  #       			m = 0
  #       			mm = 0
  #       			for nn in range(n, min(n+self.FC_TN, OUT)):
  #       				for bb in range(b, min(b+self.FC_TB, BAT)):
  #       					for ii in range(i, min(i+self.FC_TI, IN)):
        						
  #       						# multipliers[m].write(f"{input_data[bb][ii]}\t{weights[ii][nn]}\n")
  #       						m = m + 1
  #       						# adderN[mm].write(f"{input_data[bb][ii]*weights[ii][nn]}\t")
  #       					# adderN[mm].write("\n")
        					
  #       					# print(input_data[bb, i:ii+1])
  #       					# print(bb, i, ii+1, nn, n)
  #       					# # print(weights[i:ii+1])
  #       					# print(weights[i:ii+1, nn])
  #       					# accum[mm].write(f"{np.dot(input_data[bb, i:ii+1], weights[i:ii+1, nn])}\n", )
  #       					mm = mm + 1
  #       			#f.write(f"{weights[i][n]}\t{}")
  #       			print(n,b,i)
  #       for m in range(self.DRAM_LEN // self.L2_LEN):
  #       	DRAM.close()
        
  #       for m in range( max(self.L2_LEN // self.L1_ACT_LEN, self.L2_LEN // self.L1_ACT_LEN) ):
  #       	L2_buffer.close()
        
  #       #split the buffer into 16x...
  #       for m in range(max(( self.FC_TI*self.FC_TB)//self.L1_ACT_LEN , 1)):
  #       	L1_act_buffer.close()
        
  #       for m in range(max(( self.FC_TN*self.FC_TB)//self.L1_WEI_LEN, 1)):
  #       	L1_wei_buffer.close()
        
  #       for m in range(self.FC_TN):
  #       	multicast_act.close()
        
  #       for m in range(self.FC_TB):
  #       	multicast_wei.close()
        
  #       for m in range(self.FC_TN*self.FC_TB*self.FC_TI):
  #       	multipliers.close()
        	
  #       for m in range(self.FC_TN*self.FC_TB):
  #       	adderN.close()
        	
  #       for m in range(self.FC_TN*self.FC_TB* OUT*BAT):
  #       	accum.close()


#Estimate (our model using binning, toggling analysis, state-based)
#Baseline1, using average power (i.e. NeuroSim-like)
#Baseline2, using average energy (i.e. Accelergy-like)

class SimpleArch:#(AIChip):
	"""子类，实现具体的推理功能。"""

	def __init__(self, config, model_name = "Testing", run_cpp = True, np_save = True,
		RUN_PE = True, RUN_WEI_BUFFERS = True, RUN_ACT_BUFFERS = True,
		RUN_MODEL = True,
		RUN_GOLDEN=True,
		SIM_CYCLES = 88888888888888, #some gigantic number
		Randomize = False, Wei_Sparse = 0.5, Act_Sparse = 0.5,
		EDAVerification = False,

		RUN_L1 = True,
		RUN_L2 = True,
		RUN_ADDERS = True
		
		):


		self.multiplier2_radix = config['multiplier2_radix']

		self.clock = config['clock']
		self.cap_load = config['cap_load']
		self.tech = config['tech']
	
		self.RUN_PE = RUN_PE
		self.RUN_WEI_BUFFERS = RUN_WEI_BUFFERS
		self.RUN_ACT_BUFFERS = RUN_ACT_BUFFERS
		self.RUN_L1 = RUN_L1
		self.RUN_L2 = RUN_L2
		self.RUN_ADDERS = RUN_ADDERS
		
		self.RUN_GOLDEN = RUN_GOLDEN
		self.Wei_Sparse = Wei_Sparse
		self.Act_Sparse = Act_Sparse
	
		self.RUN_MODEL = RUN_MODEL
		self.np_save = np_save
		self.run_cpp = run_cpp
		self.SIM_CYCLES = SIM_CYCLES
		self.Randomize = Randomize
		self.EDAVerification = EDAVerification
		
		# CNN 相关参数
		self.CNN_TI = config["CNN_TI"]
		self.CNN_TN = config["CNN_TN"]
		self.CNN_TB = config["CNN_TB"]
		self.CNN_TNN = config["CNN_TNN"]
		self.CNN_TII = config["CNN_TII"]
		self.CNN_TBB = config["CNN_TBB"]
		
		# FC (全连接层) 相关参数
		self.FC_TI = config["FC_TI"]
		self.FC_TN = config["FC_TN"]
		self.FC_TB = config["FC_TB"]
		self.FC_TNN = config["FC_TNN"]
		self.FC_TII = config["FC_TII"]
		self.FC_TBB = config["FC_TBB"]
		self.FC_LOOP_ORDER = config["FC_LOOP_ORDER"]
		
		self.FC_PES = self.FC_TI*self.FC_TN*self.FC_TB
		self.FC_WEIS = self.FC_TI*self.FC_TN
		self.FC_ACTS = self.FC_TI*self.FC_TB
		self.FC_OUTS = self.FC_TB*self.FC_TN
		
		# AP (平均池化层) 相关参数
		self.AP_TN = config["AP_TN"]
		self.AP_TX = config["AP_TX"]
		self.AP_TY = config["AP_TY"]
		self.AP_TNN = config["AP_TNN"]
		self.AP_TXX = config["AP_TXX"]
		self.AP_TYY = config["AP_TYY"]
		
		# MX 和 MP 相关参数
		self.MP_TN = config["MP_TN"]
		self.MP_TX = config["MP_TX"]
		self.MP_TY = config["MP_TY"]
		self.MP_TNN = config["MP_TNN"]
		self.MP_TXX = config["MP_TXX"]
		self.MP_TYY = config["MP_TYY"]
		
		# ELT (Element-wise 操作) 相关参数
		self.ELT_TN = config["ELT_TN"]
		
		# ACT (激活函数) 相关参数
		self.ACT_TN = config["ACT_TN"]
		
		# SOFTMAX 相关参数
		self.SOFTMAX_TN = config["SOFTMAX_TN"]		
		# memory (by bits)
		self.WEI_PREC = config["WEI_PREC"]
		self.ACT_PREC = config["ACT_PREC"]
		
		self.DRAM_LEN = config["DRAM_LEN"]#512 
		
		self.L2_LEN = config["L2_LEN"]#256
		
		self.L1_WEI_LEN = config["L1_WEI_LEN"]#256
		self.L1_ACT_LEN = config["L1_ACT_LEN"]# 256
		
		design = [self.CNN_TI,  self.CNN_TN,  self.CNN_TB,
				self.CNN_TII, self.CNN_TNN, self.CNN_TBB,
				self.FC_TI,   self.FC_TN,   self.FC_TB,
				self.FC_TII,  self.FC_TNN,  self.FC_TBB,
				self.AP_TN,   self.AP_TX,   self.AP_TY,
				self.AP_TNN,  self.AP_TXX,  self.AP_TYY,
				self.MP_TN,   self.AP_TX,   self.AP_TY,
				self.MP_TNN,  self.AP_TXX,  self.AP_TYY,
				self.ELT_TN,  self.ACT_TN,  self.SOFTMAX_TN,
				self.DRAM_LEN, self.L2_LEN, self.L1_WEI_LEN, self.L1_ACT_LEN
				]
		design = "_".join([str(d) for d in design])
		bn = [
			SIM_CYCLES, 
			Randomize, Wei_Sparse, Act_Sparse
		]
		benchmark = "_".join([str(d) for d in bn])
		self.root = "generated/Architecture/SimpleArch/Design_"+design + "__" + model_name + "__" + benchmark
		self.log_root = "generated/Architecture/SimpleArch/logs/Design_"+design + "__" + model_name + "__" + benchmark

		if not os.path.exists("generated/Architecture/SimpleArch/logs"):
		    os.mkdir("generated/Architecture/SimpleArch/logs")	

		if not os.path.exists(self.log_root):
		    os.mkdir(self.log_root)
	
		if not os.path.exists(self.root):
		    os.mkdir(self.root)
	
	def infer_cnn(self, name, input_data, weights,out_data, stride, padding, Randomize = False):
		
		def img2col_batch(imgs, kernel_size, stride=1, padding=0):
		    """
		    批量 img2col 实现，输入形状为 (B, I, X, Y)
		    :param imgs: 输入图像，形状为 (B, I, X, Y)
		    :param kernel_size: 卷积核尺寸，如 (KX, KY)
		    :param stride: 步长，默认为 1
		    :param padding: 填充大小，默认为 0
		    :return: 列向量矩阵，形状为 (KX * KY * I, out_x * out_y * B)
		    """
		    B, I, X, Y = imgs.shape  # 批量大小、输入通道数、高度、宽度
		    KX, KY = kernel_size  # 卷积核高度和宽度
		
		    # 添加填充
		    if padding > 0:
		        imgs_padded = np.pad(imgs, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')
		    else:
		        imgs_padded = imgs
		
		    # 计算输出尺寸
		    out_x = (X + 2 * padding - KX) // stride + 1
		    out_y = (Y + 2 * padding - KY) // stride + 1
		
		    # 初始化输出矩阵
		    cols = np.zeros((KX * KY * I, out_x * out_y * B))
		
		    # 填充输出矩阵
		    for b in range(B):  # 遍历每个样本
		        img_padded = imgs_padded[b]
		        col_idx = b * out_x * out_y  # 当前样本的起始列索引
		        for i in range(0, X + 2 * padding - KX + 1, stride):
		            for j in range(0, Y + 2 * padding - KY + 1, stride):
		                patch = img_padded[:, i:i+KX, j:j+KY]  # 提取 patch，形状为 (I, KX, KY)
		                cols[:, col_idx] = patch.reshape(-1)  # 展平并填充
		                col_idx += 1
		
		    return cols
			
		kernel_size = [weights.shape[-2], weights.shape[-1]]
		stride = stride
		padding = padding
		img2col_input = img2col_batch(input_data[0], kernel_size, stride, padding)
		img2col_weights = weights.reshape((weights.shape[0], -1))
		
		print(input_data[0].shape)
		print(weights.shape)
		print(img2col_input.shape)
		print(img2col_weights.shape)
		# exit(0)
		return self.infer_fc(name, (img2col_input.reshape(img2col_input.shape).T,), img2col_weights.T,
			out_data, Randomize = Randomize)


	def infer_fc_prepare(self, name, input_data, weights, out_data, Randomize):
		#0. Prepare data
		input_data = input_data[0] #if is tuple
		IN =  weights.shape[0]
		OUT = weights.shape[1]
		BAT = input_data.shape[0]
		print(input_data.shape)
		assert(input_data.shape[1] == weights.shape[0])
		
		#quantize (fixed point)
		#(todos) some algorithm to choose the scaling ?
		weights    = ((weights*256*256) %self.WEI_PREC).astype(np.int32)
		input_data = ((input_data*256*256) %self.ACT_PREC).astype(np.int32)
		
		#Randomize weights and inputs (TODOS)
		if(Randomize):
			n = IN*OUT
			k = int(n*self.Wei_Sparse)
			rand_wei = np.random.randint(0, 256, size=n)
			zero_indices = np.random.choice(n, k, replace=False)
			rand_wei[zero_indices] = 0
			
			n = IN*BAT
			k = int(n*self.Act_Sparse)
			rand_act = np.random.randint(0, 256, size=n)
			zero_indices = np.random.choice(n, k, replace=False)
			rand_act[zero_indices] = 0
			
			weights = rand_wei.reshape((IN, OUT))
			input_data = rand_act.reshape((BAT, IN))
		
		w_file = self.root+"/"+name+".weights.txt"
		i_file = self.root+"/"+name+".input.txt"
		if(self.np_save):
			np.savetxt(w_file, weights, fmt='%d', delimiter='\n')
			np.savetxt(i_file, input_data, fmt='%d', delimiter='\n')

		return IN, OUT, BAT, w_file, i_file, weights, input_data



	#Buffer model
	# There are three layers
	# L2, L1 and DRAM possibly
	# Depending on how we split the data, there can be different modes
	# i.e. Shared L2, Different L1
	# i.e. Shared L2, Shared L1
	def infer_fc_in_buffers(self, name,
			IN, OUT, BAT, w_file, i_file, weights, input_data, output_data=None):

		def fc_buffer_toggle(prec, num_data, bank_bits, tile,
			TYPE_NAME = "",
			inner_tiles = ['TI', 'TN'],
			indexing = "i*{OUT} + n",
			buffer_loop_order = ['N',"I",'NN'],
			reset_pe = "pe",
			w_file = w_file,
			model = "sram"):
			
			OUT_FILE = self.root+"/"+name+f".{TYPE_NAME}.out"
			TRACE_FILE = self.root+"/"+name+f".{TYPE_NAME}.trace"
			TOGGLE_FILE = self.root+"/"+name+f".{TYPE_NAME}.cpp"
			JSON_FILE = self.root+"/"+name+f".{TYPE_NAME}.json"
			POWER_GOLDEN_FILE = self.root+"/"+name+f".{TYPE_NAME}.golden"
			RUNTIME = OUT_FILE+".runtime.txt" #cycles
	
			sram_banks = max(1,tile*prec//bank_bits)

			print("SRAM_banks", sram_banks)
			'''
			tile depends
			inner_tiles is the order, depends
			indexing, indexing, depends on above
			buffer_loop_order, outer loop tiling
			reset_pe, whether it is continuous indexing, 
				should be reset_pe = 0, if we are dealing with PEs (ie. multicast)
				other wise is pe = pe
			TOGGLE_FILE
			OUT_FILE
			TRACE_FILE are custom
			Assume there are no cache misses
			'''
			# print(TOGGLE_FILE)
			with open(TOGGLE_FILE, "w") as f:
				# params
				# prec = self.WEI_PREC
				# num_data = "IN*OUT"
				# bank_bits = 16 #neglect the rows, assume we have multiplexer to switch banks if 
				# #necessary
				# tile =  self.FC_WEIS #L2_LEN/self.WEI_PRECS
				# OUT_FILE = WEI_L1_PE_OUT
				# TRACE_FILE = "WEI_TRACE_FILE"
				# inner_tiles = ['TI', 'TN'] #TI, TN, TB
				# indexing = "i*{OUT} + n" #i,n,b
				# buffer_loop_order = ['N',"I",'NN']
				# reset_pe = "pe"

				# self.L2_LEN//bank_bits, L2_LEN/self.WEI_PRECS*self.WEI_PRECS//bank_bits
				# self.L1_WEI_LEN//bank_bits
				# self.L2_LEN//bank_bits, ...				
				# 64 * 8/16 = that many SRAM banks
				
				# head
				f.write("#include <iostream>\n")
				f.write("#include <algorithm>\n")
				f.write(cpp_read_file_helper())
				f.write(bit_count_helper())
				f.write("int main(){\n")
				f.write(f"int OUT = {OUT};\n")
				f.write(f"int IN = {IN};\n")
				f.write(f"int BAT = {BAT};\n")
				f.write(f"int d_size;\n")
				# f.write(f"std::cout << \"//Reading\" << std::endl;\n")
				f.write(f"int* weight = lireFichierEtRemplirTableau(\"{w_file}\", &d_size, {num_data});\n")
				# f.write(f"std::cout << \"//Reading - DONE\" << std::endl;\n")

				# neck
				#SRAM/registers we use a toggle model
				#so don't need the actual values, only need the bit counts
				f.write(f'''
					int toggle_bins[{sram_banks}][{bank_bits}];
					for (int i = 0; i < {sram_banks}; i++){{
					for (int j = 0; j < {bank_bits}; j++) {{
						toggle_bins[i][j] = 0;
					}}
				}}''')
				f.write(f'''
					int prev_val[{sram_banks}];
					for (int i = 0; i < {sram_banks}; i++){{
						prev_val[i] = 0;
					}}
				''')

				#body 
				f.write(f'std::cout << "// Analyzing Workload" << std::endl;\n')
				if(self.RUN_GOLDEN):
					f.write(f'std::ofstream goldenOutFile("{TRACE_FILE}");\n');

				f.write(f'int sim_cycles = -1;\n')	
				f.write('int pe = 0;\n')
				B_DEPTH = "BAT"
				N_DEPTH = "OUT"
				I_DEPTH = "IN"
				B_START = "0"
				N_START = "0"
				I_START = "0"
				cnt = 0
				for lp in buffer_loop_order:
					if(lp[0] == "B" and getattr(self, "FC_T"+lp) != 0):
						f.write(f"for(int {lp} = {B_START}; {lp} < {B_START+'+'+B_DEPTH}; {lp} += {getattr(self, 'FC_T'+lp)})"+"{")
						B_DEPTH = "FC_T"+lp
						B_START = lp
						cnt += 1
					elif(lp[0] == "I" and getattr(self, "FC_T"+lp) != 0):
						f.write(f"for(int {lp} = {I_START}; {lp} < {I_START+'+'+I_DEPTH}; {lp} += {getattr(self, 'FC_T'+lp)})"+"{")
						I_DEPTH = "FC_T"+lp
						I_START = lp
						cnt += 1
					elif(lp[0] == "N" and getattr(self, "FC_T"+lp) != 0):
						f.write(f"for(int {lp} = {N_START}; {lp} < {N_START+'+'+N_DEPTH}; {lp} += {getattr(self, 'FC_T'+lp)})"+"{")
						N_DEPTH = "FC_T"+lp
						N_START = lp					
						cnt += 1
				
				b_up = ""
				i_up = ""
				n_up = ""
				if("TB" in inner_tiles):
					b_up = f'''for (int b = B; b < std::min(B + {self.FC_TB}, {BAT}); b++){{'''
				if("TI" in inner_tiles):
					i_up = f'''for (int i = I; i < std::min(I + {self.FC_TI}, {IN}); i++){{'''
				if("TN" in inner_tiles):
					n_up = f'''for (int n = N; n < std::min(N + {self.FC_TN}, {OUT}); n++){{'''

				f.write(f'''
					sim_cycles++;
					if(sim_cycles > {self.SIM_CYCLES})
						break;
					pe = {reset_pe};
						 {b_up}
						 {i_up}
						 {n_up}
						int w_idx =  {indexing};
						int wb = weight[w_idx];
						
						int sram_bank = pe;
						if(sim_cycles > 0)						
						toggle_bins[sram_bank][__builtin_popcount(wb^prev_val[sram_bank])]++; 
						prev_val[sram_bank] = wb;
						//pe_bins[pe] = pe_bins[pe] + __builtin_popcount(wb^prev_val[pe]) ;
						//prev_wb[pe] = wb;
						pe = (pe + 1) % {sram_banks};
					''')

				if(self.RUN_GOLDEN):
					f.write('goldenOutFile << wb <<"\t";\n')
				
				f.write(f'''
				{len(inner_tiles)*"}"}
				''')
				
				if(self.RUN_GOLDEN):
					f.write('goldenOutFile << "\\n";\n')
				
				for i in range(cnt):
					f.write("}\n")
				
				# tail
				#SAVE RESULTS
				# f.write(f'std::cout << "// Analyzing Workload - DONE" << std::endl;\n')
				# f.write(f'std::cout << "// Saving Data" << std::endl;\n')
				f.write(f'''
					std::ofstream timeFile("{RUNTIME}");
					timeFile << sim_cycles << "\\n";
					timeFile.close();					
	
					std::ofstream outFile("{OUT_FILE}"); // 创建或覆盖文件
					if (!outFile.is_open()) {{
						std::cerr << "Failed to open file!" << std::endl;
						return 1;
					}}
					
						for (int i = 0; i < {sram_banks}; i++){{
						for (int j = 0; j < {bank_bits}; j++) {{
							outFile << toggle_bins[i][j] << "\\n";
						}}
						//outFile << "\\n";
					}}
									
					//#TODOS!!!!!!!!!!!!!!!!!!!!!!!!!!
					// for (int j = 0; j < {self.FC_WEIS}; j++) {{
					// outFile << pe_bins[j] << "\\n";
					// }}
					
					outFile.close();
					//std::cout << "// Saving PE WEI Data {OUT_FILE} - DONE" << std::endl;	
					free(weight);
					return 0;
				}}''')
			
			if(self.run_cpp):
				os.system(f"g++ -O3 {TOGGLE_FILE}") #make this as fast as possible please
				if(not IS_LINUX):
					os.system(".\\a.exe")
				else:
					os.system(f"g++ -O3 {TOGGLE_FILE} -Wl,-rpath,/nfs/project/JayMok/power_experiments_xie/primitives")
					os.system("./a.out")
		
			#we should run the power of the golden as well, as well as (todos)
			#inference of the power estimated by our power model
		
			if(self.RUN_GOLDEN):
				if(model != "multicast"):
					mode = "WRITE" in TYPE_NAME
					CONFIG = {
						"EDAVerification": self.EDAVerification,
						"CLOCK": self.clock,
						"cap_load": self.cap_load,
						"fanout_load": 0.1,
						"tech": self.tech,
						
						"sram_banks": sram_banks,
						"entry_bits": bank_bits,
						"rows": 256, #fixme
						
						"OutputPowerFile": POWER_GOLDEN_FILE				 , 
						"mode": mode
					}
					with open(JSON_FILE, "w") as json_file:
						json.dump(CONFIG, json_file, indent=4)  # indent 用于格式化输出	
					print(f'{SBT} "test:runMain memories.SRAMSSpecFromFile {TRACE_FILE} {JSON_FILE}" > golden.memory.log')
	
					os.system(f'{SBT} "test:runMain memories.SRAMSSpecFromFile {TRACE_FILE} {JSON_FILE}" > golden.memory.log')
					
					#with open(POWER_GOLDEN_FILE) as f:
					#	for l in f.readlines():
					#		print(l)
	
				else:
					CONFIG = {
						"EDAVerification": self.EDAVerification,
						"CLOCK": self.clock,
						"cap_load": self.cap_load,
						"fanout_load": 0.0,
						"tech": self.tech,
						
						"terms": 1,#tile,
						"fanout": self.FC_PES//tile,
						"prec": prec,
						
						"OutputPowerFile": POWER_GOLDEN_FILE				  
					}
					with open(JSON_FILE, "w") as json_file:
						json.dump(CONFIG, json_file, indent=4)  # indent 用于格式化输出	
					os.system(f'{SBT} "test:runMain networks.MulticastSpecFromFile {TRACE_FILE} {JSON_FILE}" > golden.multicast.log')
					#with open(POWER_GOLDEN_FILE) as f:
					#	for l in f.readlines():
					#		print(l)
					
										
			estimate_pwr = 0	
			estimate_runtime = 0
			estimate_energy = 0

			baseline1_pwr = 0	
			baseline1_runtime = 0
			baseline1_energy = 0

			baseline2_pwr = 0	
			baseline2_runtime = 0
			baseline2_energy = 0

			golden_pwr = 0
			golden_runtime = 0
			golden_energy = 0
	
			with open(RUNTIME, 'r') as f:
				estimate_runtime = int(f.readlines()[0].strip())
	
			if(self.RUN_MODEL):
				with open( OUT_FILE , "r") as f:
					toggle_bins = np.zeros((sram_banks*bank_bits))#custom in the future;
					for idx,l in enumerate(f.readlines()):
						toggle_bins[idx] = int(l)
					toggle_bins = toggle_bins.reshape((sram_banks, bank_bits))#.reshape((-1))
					
					if(model == "multicast"):
						from power_models.multicast_tester import get_multicast_LUT
						fanout = self.FC_PES//tile 
						print("fanout = ", fanout)
						power_per_toggle = get_multicast_LUT( fanout , hardware = [self.clock, self.cap_load])
					else:
						from power_models.SRAMBank_tester import get_SRAMBank_LUT
						read, write = get_SRAMBank_LUT(bank_bits, hardware = [bank_bits,sram_banks, self.clock, self.cap_load])
						if("READ" in TYPE_NAME):
							power_per_toggle = read
						else:
							power_per_toggle = write
					print("power_per_toggle", power_per_toggle)
					print("toggle_bins", toggle_bins)
					bin_power = toggle_bins*power_per_toggle
					print("bin_power", bin_power)
					SRAM_POWER = np.sum(np.sum(bin_power,axis=1))/self.SIM_CYCLES#based on multiple-pe
					print(f"Estimate SRAM_POWER (W) {TYPE_NAME} = ", SRAM_POWER)
					estimate_pwr = SRAM_POWER	

					avg_pwr = np.sum(power_per_toggle)/len(power_per_toggle)
					
					

					baseline1_pwr = tile * avg_pwr
					baseline2_pwr = baseline1_pwr#assume same for now

					if(self.RUN_GOLDEN):
						gold = pd.read_csv(POWER_GOLDEN_FILE,  delimiter="\t")
						#get relevant rows
						rel = gold.tail(n=tile)['Total_Pwr']
						print(rel, np.sum(rel))
						print("GOLDEN SRAM",tile, np.sum(rel))
						golden_pwr = np.sum(rel)
					

			res = {}
			res['name'] = TYPE_NAME

			res['estimate_pwr'] = estimate_pwr 
			res['estimate_runtime'] = estimate_runtime
			res['estimate_energy'] = estimate_energy

			res['baseline1_pwr'] = baseline1_pwr 
			res['baseline1_runtime'] = baseline1_runtime
			res['baseline1_energy'] = baseline1_energy

			res['baseline2_pwr'] = baseline2_pwr 
			res['baseline2_runtime'] = baseline2_runtime
			res['baseline2_energy'] = baseline2_energy	

			res['golden_pwr'] = golden_pwr
			res['golden_runtime'] = golden_runtime
			res['golden_energy'] = golden_energy

			return res		
		# WEI_TOGGLE, used by DRAM,DRAM-L2, L2, L2-L1, L1, L1-PE
		# WEI_TOGGLE_file = self.root+"/"+name+".WEI_TOGGLE_file.cpp"
		# WEI_L1_PE_OUT = self.root+"/"+name+".WEI_PE.out"
		# WEI_L1_TRACE_FILE = WEI_L1_PE_OUT+".trace"
		# WEI_L1_JSON_FILE = WEI_L1_PE_OUT+".json"
		# WEI_L1_POWER_GOLDEN = WEI_L1_PE_OUT+".golden.txt"		
		
		# WEI_L1_OUT = self.root+"/"+name+".WEI_L1.out"
		# WEI_L2_L1_OUT = self.root+"/"+name+".WEI_L2_L1.out"
		# WEI_L2_OUT = self.root+"/"+name+".WEI_L2.out"
		#DRAM skip for now
		
		results = []
		if(self.RUN_WEI_BUFFERS):
			prec = self.WEI_PREC
			num_data = "IN*OUT"
			bank_bits = 16
			indexing = f"i*{OUT} + n"

			if(self.RUN_L1):
				#L1 WEI WRITE
				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data, 
				bank_bits = bank_bits, 
				tile = self.L1_WEI_LEN, 
				inner_tiles = ['TI','TN'], 
				indexing = indexing,
				buffer_loop_order = [i for i in self.FC_LOOP_ORDER if 'B' not in i],
				reset_pe = "pe",
				TYPE_NAME = "L1_WEI_WRITE",
			)
				results.append(res)

				#L1 WEI READ
				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = bank_bits, 
				tile = self.L1_WEI_LEN, 
				inner_tiles = ['TI','TN'],#, 'TB'],
				indexing = indexing,
				buffer_loop_order = self.FC_LOOP_ORDER,
				reset_pe = "0",
				TYPE_NAME = "L1_WEI_READ",
			)
				results.append(res)


			#MULTICASTING NETWORK INTO PE
			res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = prec, 
				tile = self.FC_TI*self.FC_TN, 
				inner_tiles = ['TI','TN'],
				indexing = indexing,
				buffer_loop_order = self.FC_LOOP_ORDER,
				reset_pe = "0",
				TYPE_NAME = "L1_WEI_MULTICAST",
				model = "multicast"
			)
			results.append(res)


			#(TODOS) missing some interconnect between the L1 and multicasting
			# unit, L1 and L2, and L2 and DRAM (not modelled for now)
			#  for DRAM simply use a third party such as DRAMSim2 for simplicity
			# namely, a serial to parallel s2p unit
			# and if the networking is not fitting, should have a p2s unit
			
			if(self.RUN_L2):
				#L2 WEI WRITE == L2 WEI READ toggles
				#They are symmetric
				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = bank_bits, 
				tile = self.L2_LEN//prec, 
				inner_tiles = ['TI','TN'],
				indexing = indexing,
				buffer_loop_order = [i for i in self.FC_LOOP_ORDER if 'B' not in i],
				reset_pe = "pe",
				TYPE_NAME = "L2_WEI_READ",
			)
				results.append(res)


				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = bank_bits, 
				tile = self.L2_LEN//prec, 
				inner_tiles = ['TI','TN'],
				indexing = indexing,
				buffer_loop_order = [i for i in self.FC_LOOP_ORDER if 'B' not in i],
				reset_pe = "pe",
				TYPE_NAME = "L2_WEI_WRITE",
			)
				results.append(res)


		if(self.RUN_ACT_BUFFERS):
			prec = self.ACT_PREC
			num_data = "IN*BAT"
			bank_bits = 16
			indexing = f"i*{BAT} + b"

			#L1 ACT WRITE
			if(self.RUN_L1):
				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data, 
				bank_bits = bank_bits, 
				tile = self.L1_ACT_LEN, 
				inner_tiles = ['TI','TB'], 
				indexing = indexing,
				buffer_loop_order = [i for i in self.FC_LOOP_ORDER if 'N' not in i],
				reset_pe = "pe",
				
				TYPE_NAME = "L1_ACT_WRITE",

				w_file = i_file
			)
				results.append(res)


				#L1 WEI READ
				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = bank_bits, 
				tile = self.L1_WEI_LEN, 
				inner_tiles = ['TI', 'TB'],#['TI','TN', 'TB'],
				indexing = indexing,
				buffer_loop_order = self.FC_LOOP_ORDER,
				reset_pe = "0",
				
				TYPE_NAME = "L1_ACT_READ",

				w_file = i_file
			)
				results.append(res)


			#MULTICASTING NETWORK INTO PE
			res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = prec, 
				tile = self.FC_TI*self.FC_TB, 
				inner_tiles = ['TI', 'TB'],
				indexing = indexing,
				buffer_loop_order = self.FC_LOOP_ORDER,
				reset_pe = "0",
				TYPE_NAME = "L1_ACT_MULTICAST",
				model = "multicast",
				w_file=i_file
	
			)
			results.append(res)


			if(self.RUN_L2):
				#L2 WEI WRITE == L2 WEI READ toggles
				#They are symmetric
				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = bank_bits, 
				tile = self.L2_LEN//prec, 
				inner_tiles = ['TI','TB'],
				indexing = indexing,
				buffer_loop_order = [i for i in self.FC_LOOP_ORDER if 'N' not in i],
				reset_pe = "pe",
				
				TYPE_NAME = "L2_ACT_READ",
				w_file=i_file
			)
				results.append(res)

	
				res = fc_buffer_toggle(
				prec=prec, 
				num_data = num_data,
				bank_bits = bank_bits, 
				tile = self.L2_LEN//prec, 
				inner_tiles = ['TI','TB'],
				indexing = indexing,
				buffer_loop_order = [i for i in self.FC_LOOP_ORDER if 'N' not in i],
				reset_pe = "pe",
				
				TYPE_NAME = "L2_ACT_WRITE",
					w_file=i_file
			
			)
				results.append(res)


		# (TODOS, for the output buffer data, i.e.)
		# if(self.RUN_OUT_BUFFERS):
		# 	pass
		
		return results#runtime, power
				
	def infer_fc_mult(self, name,
			IN, OUT, BAT, w_file, i_file, weights, input_data):
		#PE
		PE_file = self.root+"/"+name+".PE.cpp"
		PE_OUT_FILE = self.root+"/"+name+".PE.out"
		PE_TRACE_FILE_0 = PE_OUT_FILE+".trace.in_0"
		PE_TRACE_FILE_1 = PE_OUT_FILE+".trace.in_1"
		PE_JSON_FILE = PE_OUT_FILE+".json"
		PE_POWER_GOLDEN = PE_OUT_FILE+".golden.txt"
		PE_RUNTIME = PE_OUT_FILE+".runtime.txt" #cycles

		estimate_pwr = 0	
		estimate_runtime = 0
		estimate_energy = 0
		golden_pwr = 0
		golden_runtime = 0
		golden_energy = 0
	
		baseline1_pwr = 0	
		baseline1_runtime = 0
		baseline1_energy = 0

		baseline2_pwr = 0	
		baseline2_runtime = 0
		baseline2_energy = 0



		if(self.RUN_PE):
			with open(PE_file, "w") as f:
				f.write("#include <iostream>\n")
				f.write(cpp_read_file_helper())
				f.write(bit_count_helper())
				
				f.write("int main(){\n")
				f.write(f"int OUT = {OUT};\n")
				f.write(f"int IN = {IN};\n")
				f.write(f"int BAT = {BAT};\n")
				f.write(f"int w_size;\n")
				f.write(f"int i_size;\n")
				f.write(f"std::cout << \"//Reading Weights and Inputs\" << std::endl;\n")
				f.write(f"int* weight = lireFichierEtRemplirTableau(\"{w_file}\", &w_size, IN*OUT);\n")
				f.write(f"int* input = lireFichierEtRemplirTableau(\"{i_file}\", &i_size, BAT*IN);\n") 
				f.write(f"std::cout << \"//Reading Weights and Inputs - DONE\" << std::endl;\n")
					
				#Compressed Model, convert A B, to bit counts
				# f.write(f"int* weight_bits = countBitsInArray(weight, w_size);\n")
				# f.write(f"int* input_bits = countBitsInArray(input, i_size);\n")
				
				#Traverse unroll loop order
				# f.write(f"int* bins = (int*) malloc(sizeof(int)*256*256);\n")
				f.write("""
				int bins[256][256];
				for (int i = 0; i < 256; i++) {
					for (int j = 0; j < 256; j++) {
						bins[i][j] = 0;
					}
				}
				""")
				B_DEPTH = "BAT"
				N_DEPTH = "OUT"
				I_DEPTH = "IN"
				B_START = "0"
				N_START = "0"
				I_START = "0"
				
				cnt = 3
				#Don't need real loop
				f.write(f'std::cout << "// Analyzing Workload" << std::endl;\n')
				if(self.RUN_GOLDEN):
					f.write(f'std::ofstream goldenOutFile0("{PE_TRACE_FILE_0}");\n');
					f.write(f'std::ofstream goldenOutFile1("{PE_TRACE_FILE_1}");\n');
					
				f.write(f'''
				int sim_cycles = 0;
				for(int B = 0; B < {B_DEPTH} ; B++){{
					for(int N = 0; N < {OUT}; N++){{
						for(int I = 0; I <{IN}; I++){{
							sim_cycles++;
							if(sim_cycles > {self.SIM_CYCLES})
							break;
				''')

				f.write(f'''
					int w_idx = I*{OUT} + N;
					int i_idx = B*{BAT} + I;
					int wb = weight[w_idx];//weight_bits[w_idx];
					int ib = input[i_idx];//input_bits[i_idx];
					bins[wb][ib  ] = bins[wb][ib] + 1;
				''')
				
				if(self.RUN_GOLDEN):
					f.write('goldenOutFile0 << wb << "\\n";\n')
					f.write('goldenOutFile1 << ib << "\\n";\n')

				for lp in range(cnt):
					f.write("}\n")
				f.write(f'std::cout << "// Analyzing Workload - DONE" << std::endl;\n')
				f.write(f'std::cout << "// Saving Data" << std::endl;\n')
				
				if(self.RUN_GOLDEN):
					f.write(f'goldenOutFile0.close();\n')
					f.write(f'goldenOutFile1.close();\n')
				
				f.write(f'''
					std::ofstream timeFile("{PE_RUNTIME}");
					timeFile << sim_cycles << "\\n";
					timeFile.close();					
					std::ofstream outFile("{PE_OUT_FILE}"); // 创建或覆盖文件
					if (!outFile.is_open()) {{
						std::cerr << "Failed to open file!" << std::endl;
						return 1;
					}}
					for(int wb = 0; wb < 256; wb ++){{
						for(int ib = 0 ; ib < 256; ib++){{
							outFile << bins[wb][ib] << "\\n"; //bins[wb][ib]
						}}
					}}
					outFile.close();
					std::cout << "// Saving Data - DONE" << std::endl;	
					free(weight);
					free(input);
					return 0;
				}}''')
		
			if(self.run_cpp):
				TOGGLE_FILE = PE_file
				os.system(f"g++ {PE_file}")
				if(not IS_LINUX):
					os.system(".\\a.exe")
				else:
					os.system(f"g++ -O3 {TOGGLE_FILE} -Wl,-rpath,/nfs/project/JayMok/power_experiments_xie/primitives")	
					os.system("./a.out")
	
				#os.system(".\\a.exe")
			
			
		
			if(self.RUN_GOLDEN):
				MULT_CONFIG = {
					
					"EDAVerification": self.EDAVerification,
					
					"MULTS": 2,
					"CLOCK": self.clock,
					"cap_load": self.cap_load,
					"fanout_load": 0.1,
					
					"tech": self.tech, #"tsmc40",
					
					"radix": self.multiplier2_radix,
					"multiplierType": "HighRadixMultiplier",
					"side": "None",
					"prec2":self.ACT_PREC,
					"prec1":self.WEI_PREC,
					"adderType":"SimpleAdder2",
					
					"OutputPowerFile": PE_POWER_GOLDEN
				}
				with open(PE_JSON_FILE, "w") as json_file:
					json.dump(MULT_CONFIG, json_file, indent=4)  # indent 用于格式化输出
				# print(f'sbt "test:runMain multipliers.Multiplier2SpecFromFile {PE_TRACE_FILE} {PE_JSON_FILE}"')

				os.system(f'{SBT} "test:runMain multipliers.Multiplier2SpecFromFile {PE_TRACE_FILE_0} {PE_TRACE_FILE_1} {PE_JSON_FILE}" > golden.mult.log')
				POWER_GOLDEN_FILE = PE_POWER_GOLDEN
				with open(POWER_GOLDEN_FILE) as f:
					for l in f.readlines():
						#print(l)
						pass
	

			if(self.RUN_MODEL):
				with open(PE_RUNTIME, 'r') as f:
					estimate_runtime = int(f.readlines()[0].strip())/self.FC_PES
	
				with open(PE_OUT_FILE, "r") as f:
					pe_bins = np.zeros((256*256))#custom in the future
					for idx,l in enumerate(f.readlines()):
						pe_bins[idx] = int(l)
					# pe_bins = pe_bins.reshape((256,256)).T.reshape((-1))
					
					from power_models.multiplier2_tester import get_multiplier2_LUT, multiplier2_infer_sequence
					mult_power = get_multiplier2_LUT(self.WEI_PREC,
						hardware = [self.clock, self.multiplier2_radix,
							self.clock, self.cap_load])
					
					bin_power = mult_power * pe_bins
					# print(bin_power)
					# print(np.sum(bin_power))
					PE_POWER = self.FC_PES*np.sum(bin_power)/self.SIM_CYCLES#based on multiple-pe
					estimate_pwr = PE_POWER
					#PE_POWER = multiplier2_infer_sequence(hardware=[self.clock, ], seq)

					power_per_toggle = mult_power
					avg_pwr = np.sum(power_per_toggle)/len(power_per_toggle)
					baseline1_pwr = self.FC_PES * avg_pwr
					baseline2_pwr = baseline1_pwr#assume same for now


					print("Estimate Total PE_POWER = ", PE_POWER)
					if(self.RUN_GOLDEN):
						gold = pd.read_csv(POWER_GOLDEN_FILE,  delimiter="\t")
						#get relevant rows
						rel = gold.tail(n=1)['Total_Pwr']
						#	print(gold.tail(n=1))
						golden_pwr = self.FC_PES*np.sum(rel)
						print("Golden PE_POWER = ", self.FC_PES*np.sum(rel))
						
	
	
		res = {}
		res['name'] = 'PE'

		res['baseline1_pwr'] = baseline1_pwr 
		res['baseline1_runtime'] = baseline1_runtime
		res['baseline1_energy'] = baseline1_energy

		res['baseline2_pwr'] = baseline2_pwr 
		res['baseline2_runtime'] = baseline2_runtime
		res['baseline2_energy'] = baseline2_energy
	
		res['estimate_pwr'] = estimate_pwr 
		res['estimate_runtime'] = estimate_runtime
		res['estimate_energy'] = estimate_energy

		res['golden_pwr'] = golden_pwr
		res['golden_runtime'] = golden_runtime
		res['golden_energy'] = golden_energy

		return [res]
	"""		
	def infer_fc_adder_tree(name,
			IN, OUT, BAT, w_file, i_file, weights, input_data):
		## todos
		## need to multiply terms and collect them together to generate

		buffer_loop_order = self.FC_LOOP_ORDER,
		inner_tiles = ['TI','TB', 'TN']	
		prec = self.WEI_PREC + self.ACT_PREC	

		OUT_FILE = self.root+"/"+name+f".{TYPE_NAME}.out"
		TRACE_FILE = self.root+"/"+name+f".{TYPE_NAME}.trace"
		TOGGLE_FILE = self.root+"/"+name+f".{TYPE_NAME}.cpp"
		JSON_FILE = self.root+"/"+name+f".{TYPE_NAME}.json"
		POWER_GOLDEN_FILE = self.root+"/"+name+f".{TYPE_NAME}.golden"
		RUNTIME = OUT_FILE+".runtime.txt" #cycles
	
		with open(TOGGLE_FILE, "w") as f:
			f.write("#include <iostream>\n")
			f.write("#include <algorithm>\n")
			f.write(cpp_read_file_helper())
			f.write(bit_count_helper())
			f.write("int main(){\n")
			f.write(f"int OUT = {OUT};\n")
			f.write(f"int IN = {IN};\n")
			f.write(f"int BAT = {BAT};\n")
			f.write(f"int d_size;\n")
			f.write(f"int* weight = lireFichierEtRemplirTableau(\"{w_file}\", &w_size, IN*OUT);\n")
			f.write(f"int* input = lireFichierEtRemplirTableau(\"{i_file}\", &i_size, BAT*IN);\n") 
	
			# neck
			f.write(f'''
			int toggle_bins[{self.FC_PES}][{prec}];
			for (int k = 0; k < {self.FC_PES}; k++)
				for (int j = 0; j < {prec}; j++) {{
					toggle_bins[k][j] = 0;
			}}''')

				f.write(f'''
					int prev_val[{self.FC_PES}][{prec}];
					for(int k = 0; k < {self.FC_PES}; k++)
					for (int i = 0; i < {prec}; i++){{
						prev_val[k][i] = 0;
					}}
				''')

				#body 
				f.write(f'std::cout << "// Analyzing Workload" << std::endl;\n')
				if(self.RUN_GOLDEN):
					f.write(f'std::ofstream goldenOutFile("{TRACE_FILE}");\n');

				f.write(f'int sim_cycles = -1;\n')	
				f.write('int pe = 0;\n')
				B_DEPTH = "BAT"
				N_DEPTH = "OUT"
				I_DEPTH = "IN"
				B_START = "0"
				N_START = "0"
				I_START = "0"
				cnt = 0
				for lp in buffer_loop_order:
					if(lp[0] == "B" and getattr(self, "FC_T"+lp) != 0):
						f.write(f"for(int {lp} = {B_START}; {lp} < {B_START+'+'+B_DEPTH}; {lp} += {getattr(self, 'FC_T'+lp)})"+"{")
						B_DEPTH = "FC_T"+lp
						B_START = lp
						cnt += 1
					elif(lp[0] == "I" and getattr(self, "FC_T"+lp) != 0):
						f.write(f"for(int {lp} = {I_START}; {lp} < {I_START+'+'+I_DEPTH}; {lp} += {getattr(self, 'FC_T'+lp)})"+"{")
						I_DEPTH = "FC_T"+lp
						I_START = lp
						cnt += 1
					elif(lp[0] == "N" and getattr(self, "FC_T"+lp) != 0):
						f.write(f"for(int {lp} = {N_START}; {lp} < {N_START+'+'+N_DEPTH}; {lp} += {getattr(self, 'FC_T'+lp)})"+"{")
						N_DEPTH = "FC_T"+lp
						N_START = lp					
						cnt += 1
				
				b_up = ""
				i_up = ""
				n_up = ""
				if("TB" in inner_tiles):
					b_up = f'''for (int b = B; b < std::min(B + {self.FC_TB}, {BAT}); b++){{'''
				if("TI" in inner_tiles):
					i_up = f'''for (int i = I; i < std::min(I + {self.FC_TI}, {IN}); i++){{'''
				if("TN" in inner_tiles):
					n_up = f'''for (int n = N; n < std::min(N + {self.FC_TN}, {OUT}); n++){{'''

				f.write(f'''
					sim_cycles++;
					if(sim_cycles > {self.SIM_CYCLES})
						break;

					pe = 0;
						 {b_up}
						 {n_up}
						 {i_up}	
						int w_idx = i*{OUT} + n;
						int i_idx = b*{BAT} + i;
						int wb = weight[w_idx];//weight_bits[w_idx];
						int ib = input[i_idx];//input_bits[i_idx];	
						
						if(sim_cycles > 0)						
							toggle_bins[pe][__builtin_popcount(wb^prev_val[pe])]++; 
						prev_val[sram_bank] = wb*ib;
						pe = (pe + 1);
					''')

				if(self.RUN_GOLDEN):
					f.write('goldenOutFile << wb <<"\t";\n')
				
				f.write(f'''
				{len(inner_tiles)*"}"}
				''')
				
				if(self.RUN_GOLDEN):
					f.write('goldenOutFile << "\\n";\n')
				
				for i in range(cnt):
					f.write("}\n")
				
				# tail
				#SAVE RESULTS
				# f.write(f'std::cout << "// Analyzing Workload - DONE" << std::endl;\n')
				# f.write(f'std::cout << "// Saving Data" << std::endl;\n')
				f.write(f'''
					std::ofstream timeFile("{RUNTIME}");
					timeFile << sim_cycles << "\\n";
					timeFile.close();					
	
					std::ofstream outFile("{OUT_FILE}"); // 创建或覆盖文件
					if (!outFile.is_open()) {{
						std::cerr << "Failed to open file!" << std::endl;
						return 1;
					}}
					
						for (int i = 0; i < {sram_banks}; i++){{
						for (int j = 0; j < {bank_bits}; j++) {{
							outFile << toggle_bins[i][j] << "\\n";
						}}
						//outFile << "\\n";
					}}
									
					//#TODOS!!!!!!!!!!!!!!!!!!!!!!!!!!
					// for (int j = 0; j < {self.FC_WEIS}; j++) {{
					// outFile << pe_bins[j] << "\\n";
					// }}
					
					outFile.close();
					//std::cout << "// Saving PE WEI Data {OUT_FILE} - DONE" << std::endl;	
					free(weight);
					return 0;
				}}''')
			
			if(self.run_cpp):
				os.system(f"g++ -O3 {TOGGLE_FILE}") #make this as fast as possible please
				if(not IS_LINUX):
					os.system(".\\a.exe")
				else:
					os.system(f"g++ -O3 {TOGGLE_FILE} -Wl,-rpath,/nfs/project/JayMok/power_experiments_xie/primitives")
					os.system("./a.out")
		
			#we should run the power of the golden as well, as well as (todos)
			#inference of the power estimated by our power model
		
			if(self.RUN_GOLDEN):
				if(model != "multicast"):
					mode = "WRITE" in TYPE_NAME
					CONFIG = {
						"EDAVerification": self.EDAVerification,
						"CLOCK": self.clock,
						"cap_load": self.cap_load,
						"fanout_load": 0.1,
						"tech": self.tech,
						
						"sram_banks": sram_banks,
						"entry_bits": bank_bits,
						"rows": 256, #fixme
						
						"OutputPowerFile": POWER_GOLDEN_FILE				 , 
						"mode": mode
					}
					with open(JSON_FILE, "w") as json_file:
						json.dump(CONFIG, json_file, indent=4)  # indent 用于格式化输出	
					print(f'{SBT} "test:runMain memories.SRAMSSpecFromFile {TRACE_FILE} {JSON_FILE}" > golden.memory.log')
	
					os.system(f'{SBT} "test:runMain memories.SRAMSSpecFromFile {TRACE_FILE} {JSON_FILE}" > golden.memory.log')
					
					#with open(POWER_GOLDEN_FILE) as f:
					#	for l in f.readlines():
					#		print(l)
	
				else:
					CONFIG = {
						"EDAVerification": self.EDAVerification,
						"CLOCK": self.clock,
						"cap_load": self.cap_load,
						"fanout_load": 0.0,
						"tech": self.tech,
						
						"terms": 1,#tile,
						"fanout": self.FC_PES//tile,
						"prec": prec,
						
						"OutputPowerFile": POWER_GOLDEN_FILE				  
					}
					with open(JSON_FILE, "w") as json_file:
						json.dump(CONFIG, json_file, indent=4)  # indent 用于格式化输出	
					os.system(f'{SBT} "test:runMain networks.MulticastSpecFromFile {TRACE_FILE} {JSON_FILE}" > golden.multicast.log')
					#with open(POWER_GOLDEN_FILE) as f:
					#	for l in f.readlines():
					#		print(l)
					
										
			estimate_pwr = 0	
			estimate_runtime = 0
			estimate_energy = 0

			baseline1_pwr = 0	
			baseline1_runtime = 0
			baseline1_energy = 0

			baseline2_pwr = 0	
			baseline2_runtime = 0
			baseline2_energy = 0

			golden_pwr = 0
			golden_runtime = 0
			golden_energy = 0
	
			with open(RUNTIME, 'r') as f:
				estimate_runtime = int(f.readlines()[0].strip())
	
			if(self.RUN_MODEL):
				with open( OUT_FILE , "r") as f:
					toggle_bins = np.zeros((sram_banks*bank_bits))#custom in the future;
					for idx,l in enumerate(f.readlines()):
						toggle_bins[idx] = int(l)
					toggle_bins = toggle_bins.reshape((sram_banks, bank_bits))#.reshape((-1))
					
					if(model == "multicast"):
						from power_models.multicast_tester import get_multicast_LUT
						fanout = self.FC_PES//tile 
						print("fanout = ", fanout)
						power_per_toggle = get_multicast_LUT( fanout , hardware = [self.clock, self.cap_load])
					else:
						from power_models.SRAMBank_tester import get_SRAMBank_LUT
						read, write = get_SRAMBank_LUT(bank_bits, hardware = [bank_bits,sram_banks, self.clock, self.cap_load])
						if("READ" in TYPE_NAME):
							power_per_toggle = read
						else:
							power_per_toggle = write
					print("power_per_toggle", power_per_toggle)
					print("toggle_bins", toggle_bins)
					bin_power = toggle_bins*power_per_toggle
					print("bin_power", bin_power)
					SRAM_POWER = np.sum(np.sum(bin_power,axis=1))/self.SIM_CYCLES#based on multiple-pe
					print(f"Estimate SRAM_POWER (W) {TYPE_NAME} = ", SRAM_POWER)
					estimate_pwr = SRAM_POWER	

					avg_pwr = np.sum(power_per_toggle)/len(power_per_toggle)
					
					

					baseline1_pwr = tile * avg_pwr
					baseline2_pwr = baseline1_pwr#assume same for now

					if(self.RUN_GOLDEN):
						gold = pd.read_csv(POWER_GOLDEN_FILE,  delimiter="\t")
						#get relevant rows
						rel = gold.tail(n=tile)['Total_Pwr']
						print(rel, np.sum(rel))
						print("GOLDEN SRAM",tile, np.sum(rel))
						golden_pwr = np.sum(rel)
					

			res = {}
			res['name'] = TYPE_NAME

			res['estimate_pwr'] = estimate_pwr 
			res['estimate_runtime'] = estimate_runtime
			res['estimate_energy'] = estimate_energy

			res['baseline1_pwr'] = baseline1_pwr 
			res['baseline1_runtime'] = baseline1_runtime
			res['baseline1_energy'] = baseline1_energy

			res['baseline2_pwr'] = baseline2_pwr 
			res['baseline2_runtime'] = baseline2_runtime
			res['baseline2_energy'] = baseline2_energy	

			res['golden_pwr'] = golden_pwr
			res['golden_runtime'] = golden_runtime
			res['golden_energy'] = golden_energy

			return res		
	


		
		runtime = 0
		power = 0
		return runtime, power
	"""


	def infer_fc_accumulator():
		pass	

	def infer_fc(self, name, input_data, weights, out_data, Randomize = False):
		"""
		实现全连接层操作。
		:param input_data: 输入数据，形状为 (B, I)
		:return: 全连接结果，形状为 (B, N)
		#0. Workflow
		#1. Create C++ (for different modeled components)
		#2. Run C++ (g++ ./C_file.cpp)
		#3. Read in the C++ file (i.e. bins) and run the power model
		#4. (if enabled verification) Create Scala Chisel code
		#5. (Run Scala + vcd generated power model)
		#6. (if enabled comparison) Compare Chisel vs. C++ model
		"""
		
		IN, OUT, BAT, w_file, i_file, weights, input_data = self.infer_fc_prepare(name, input_data, weights, out_data,Randomize)
		#1. Create C++
		
		b_res = self.infer_fc_in_buffers(name,
			IN, OUT, BAT, w_file, i_file, weights, input_data, out_data)
		m_res = self.infer_fc_mult(name, IN, OUT, BAT, w_file, i_file, weights, input_data)
		#a_names, a_golden, a_estimate = self.infer_fc_adder_accum(name, IN, OUT, BAT, w_file, i_file, weights, input_data)
		
		#OUT (re-scaling)
		#OUT-L1
		#L1
		#L1-L2
		#L2
		#L2-DRAM
		
		results = []
		results = results + m_res
		results = results + b_res
		return results	

  #   def infer_avg_pool(self, input_data):
  #       """
  #       实现平均池化操作。
  #       :param input_data: 输入数据，形状为 (B, I, H, W)
  #       :return: 池化结果，形状为 (B, I, H//2, W//2)
  #       """
  #       B, I, H, W = input_data.shape
  #       output = np.zeros((B, I, H // 2, W // 2))

  #       for b in range(B):
  #           for i in range(I):
  #               output[b, i] = input_data[b, i].reshape(H // 2, 2, W // 2, 2).mean(axis=(1, 3))

  #       return output

  #   def infer_max_pool(self, input_data):
  #       """
  #       实现最大池化操作。
  #       :param input_data: 输入数据，形状为 (B, I, H, W)
  #       :return: 池化结果，形状为 (B, I, H//2, W//2)
  #       """
  #       B, I, H, W = input_data.shape
  #       output = np.zeros((B, I, H // 2, W // 2))

  #       for b in range(B):
  #           for i in range(I):
  #               output[b, i] = input_data[b, i].reshape(H // 2, 2, W // 2, 2).max(axis=(1, 3))

  #       return output

  #   def infer_act(self, input_data, name = 'activation'):
  #       """
  #       实现激活函数（ReLU）。
  #       :param input_data: 输入数据
  #       :return: 激活后的数据
  #       """
  #       return np.maximum(0, input_data)
  #   def infer_element_add(self, input_data1, input_data2, name = 'element_add'):
  #       """
		# 实现元素相加操作。
		# :param input_data1: 输入数据 1
		# :param input_data2: 输入数据 2
		# :return: 相加结果
		# """
  #       for m in range(self.ELT_TN):
  #       	adder2.append(open(f"{self.root}/{name}/adder2_{m}.txt","w"))
		
  #       for m in range(max(1, self.ELT_TN//self.L1_ACT_LEN)):
  #       	L1_in1.append(open(f"{self.root}/{name}/L1_in1_{m}.txt","w"))
		
  #       for m in range(max(1, self.ELT_TN//self.L1_ACT_LEN)):
  #       	L1_in2.append(open(f"{self.root}/{name}/L1_in2_{m}.txt","w"))
		
  #       #L1_act
  #       for n in range(0, OUT, self.ELT_TN):
  #           for nn in range(n, min(n+self.ELT_TN, OUT)):
  #               pass
				
  #       #L1_wei
  #       for n in range(0, OUT, self.ELT_TN):
  #           for nn in range(n, min(n+self.ELT_TN, OUT)):
  #               pass
				
		
  #       for n in range(0, OUT, self.ELT_TN):
  #           for nn in range(n, min(n+self.ELT_TN, OUT)):
  #               pass
				
  #       for m in range(self.ELT_TN):
  #           adder2[m].close()
			
        			

  #   def infer_softmax(self, input_data):
  #       """
  #       实现 Softmax 操作。
  #       :param input_data: 输入数据，形状为 (B, N)
  #       :return: Softmax 结果，形状为 (B, N)
  #       """
  #       exp_data = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))
  #       return exp_data / np.sum(exp_data, axis=1, keepdims=True)

  #   #change to scala to infer the results
  #   # def infer_network_golden(self, intermediate_outputs):
  #   #     for layer_name, layer, input_data, out_data in intermediate_outputs:
  #   #         elif isinstance(layer, nn.Linear):
  #   #             params = {
  #   #                 "in_features": layer.weight.shape[1],
  #   #             	"out_features": layer.weight.shape[0]
  #   #             }
  #   #             self.infer_fc(input_data, layer.weight)

	#this one uses our power models
	def infer_network(self,intermediate_outputs):
		for layer_name, layer, input_data, out_data in intermediate_outputs:
			if isinstance(layer, nn.Conv2D):
				params = {
					"in_channels": layer._in_channels,
					"out_channels": layer._out_channels,
					"kernel_size": layer._kernel_size,
					"stride": layer._stride,
					"padding": layer._padding
				}
				#continue
				results = self.infer_cnn(layer_name, input_data, layer.weight, out_data,
					stride = layer._stride[0],
					padding = layer._padding,
					Randomize=self.Randomize)
							
				self.save_results(layer_name,results)

			elif isinstance(layer, nn.Linear):
				params = {
					"in_features": layer.weight.shape[1],
					"out_features": layer.weight.shape[0]
				}
				results = self.infer_fc(layer_name, input_data, layer.weight, out_data, Randomize=self.Randomize)
		
				self.save_results(layer_name,results)
			print(f"Layer: {layer_name}")
			#todos
			#should identify layer and then send to the infer as above
			#the infer above will output two things:
			#1. traces
			#2. power estimated from trace (use our model and other models)
			#3. ground truth from traces (use Scala + ptpx flow)
	
	def save_results(self,layer_name, results):

			

		head = []
		line = []
		baseline1_pwr = 0
		baseline2_pwr = 0
		estimate_pwr = 0
		golden_pwr = 0
	
		vals = ['baseline1_pwr', 'baseline2_pwr', 'estimate_pwr', 'golden_pwr']
		#(todos) baseline_runtime, energy, area
		accum = [0]*len(vals)
		for res in results:
			#baseline1_pwr += res['baseline1_pwr']	
			#baseline2_pwr += res['baseline2_pwr']	
			#estimate_pwr += res['estimate_pwr']	
			#golden_pwr += res['golden_pwr']	

			for idx in range(len(accum)):
				accum[idx] += res[vals[idx]]

			head += [res['name']+"_"+v   for v in vals ]
			line += [res[v]   for v in vals]
		
		head += ['total_'+v for v in vals]
		line += accum	


		with open(self.log_root+'/'+layer_name+'.txt','w') as f:
			f.write('\t'.join(head) + '\n')
			f.write('\t'.join([str(l) for l in line]) + '\n')
	
	
import benchmarks.vision.vision as vision
from paddle.vision.models import LeNet, alexnet,AlexNet
from paddle.vision.models import vgg16, resnet50, resnet18, resnet34, resnet101, resnet152


#SimpleArch
# Same Mapping + Hardware for all inference layers
#	meaning no multi-precision, reconfigurable, systolic, sparsity, winograd
#	but we can have bit-serial inference
#	Convolution is reduced to Fully-connected Layer, this can have toll on the energy for L1
# Pick and Choose Layers to Infer
# Generate Power, Energy, Runtime (TODOS) Delay (i.e. for Timing), Area
if __name__ == "__main__2":
    #benchmarks
	#lenet
	#alexnet
	#resnet
	
	#synthetic tests
	#1. get benchmarks say vision
	#2. for each benchmark, iterate each layer
	#3. corresponding infer --> get traces
	
	#OPTIMIZATION
	#1. PREPARE
	#2. INFER POWER
	
	images = ["src/test/resources/gou.jpg","src/test/resources/qiche.jpg",
		"src/test/resources/gou.jpg","src/test/resources/qiche.jpg"]
	model = alexnet(pretrained=True)#alexnet(pretrained=True)
	intermediate,output = vision.get_intermediate_output(images, model)
	# vision.check_labels(output)

	#3. corresponding infer --> get traces
	config = {
        # CNN 相关参数
        "CNN_TI" : 8,
        "CNN_TN" : 8, 
        "CNN_TB" : 1, 
        "CNN_TNN" : 0,
        "CNN_TII" : 0,
        "CNN_TBB" : 0,
		"CNN_TX" : 1,
		"CNN_TY" : 1,
		"CNN_TXX": 0,
		"CNN_TYY": 0,
		"CNN_TKX": 1,
		"CNN_TKY": 1,
        
        # FC (全连接层) 相关参数
        "FC_TI"  : 16 ,
        "FC_TN"  : 16 ,
        "FC_TB"  : 1 ,
        "FC_TNN" : 0 ,
        "FC_TII" : 0 ,
        "FC_TBB" : 0 ,
		"FC_LOOP_ORDER" : ["B", "N", "I"],#["B","N","I"],#["B","BB","N","I","II","NN"],
        
        # AP (平均池化层) 相关参数
        "AP_TN" :  1, 
        "AP_TX" :  1, 
        "AP_TY" :  1, 
        "AP_TNN" : 0,
        "AP_TXX" : 0,
        "AP_TYY" : 0,
        # MX 和 MP 相关参数
        "MP_TN" :  1, 
        "MP_TX" :  1, 
        "MP_TY" :  1, 
        "MP_TNN" : 0,
        "MP_TXX" : 0,
        "MP_TYY" : 0,
				
        # ELT (Element-wise 操作) 相关参数
        "ELT_TN" : 8, 
        
        # ACT (激活函数) 相关参数
        "ACT_TN" : 8,
        
        # SOFTMAX 相关参数
        "SOFTMAX_TN" : 8,
	}
	
	config.update({
        # memory (by bits)
        "WEI_PREC": 8,#(todos) make editable
        "ACT_PREC": 8,
        
        "DRAM_LEN": 512, #bits 
        
        "L2_LEN" : 64*8, #bits
        
        "L1_WEI_LEN" : config["FC_TI"]*config["FC_TN"]*8, #bits
        "L1_ACT_LEN" : config["FC_TI"]*config["FC_TB"]*8, #bits


		"multiplier2_radix": 4,
		
		
		"clock": 1,
		"cap_load": 0.1,
		"tech": "tsmc40",
	})
	
	sa = SimpleArch(config, model_name = model.__class__.__name__.lower(), 
		np_save = 1, 
		run_cpp = 1, 

		RUN_PE=1,
		RUN_WEI_BUFFERS=1,
		RUN_ACT_BUFFERS=1,
		RUN_ADDERS=1,

		RUN_MODEL =1,
		RUN_GOLDEN=1,
		RUN_L1 = 0,
		RUN_L2 = 0,
	
		SIM_CYCLES = 10, Randomize = True,
		EDAVerification = True,#False,
		Wei_Sparse = 0.5, Act_Sparse = 0.8 ) #0 more sparse, 1 less sparse
	sa.infer_network(intermediate)
	
	#4. --> predict the powers using "chisel" vs. "our model"
	#sa.get_golden(intermediate)
	
	#5. plot accuracy
	#sa.plot_model()
	
	


if __name__ == "__main__":
    #benchmarks
	#lenet
	#alexnet
	#resnet
	
	#synthetic tests
	#1. get benchmarks say vision
	#2. for each benchmark, iterate each layer
	#3. corresponding infer --> get traces
	
	#OPTIMIZATION
	#1. PREPARE
	#2. INFER POWER
	
	images = ["src/test/resources/gou.jpg","src/test/resources/qiche.jpg",
		"src/test/resources/gou.jpg","src/test/resources/qiche.jpg"]
	model = alexnet(pretrained=True)#alexnet(pretrained=True)
	intermediate,output = vision.get_intermediate_output(images, model)
	# vision.check_labels(output)

	combos = []
	for ti in [1, 2, 4, 8, 16]:
		for tn in [1,2,4,8, 16]:
			for tb in [1,2,4]:
				for lp in [["N", "I", "B"], ["I", "N", "B"], ["B", "I", "N"], ["B", "N", "I"], ["I", "N", "B"], ["I", "B", "N"]]:
					for radix in [2,4,16]:
						for ws in [0.1, 0.5, 0.9]:
							for acts in [0.1, 0.5, 0.9]:
								combos.append([ti,tn,tb,lp, radix, ws, acts])								

	for c in [combos[0]]:
		ti, tn, tb, lp, radix, ws, acts = c

		#3. corresponding infer --> get traces
		config = {
        # CNN 相关参数
        "CNN_TI" : 8,
        "CNN_TN" : 8, 
        "CNN_TB" : 1, 
        "CNN_TNN" : 0,
        "CNN_TII" : 0,
        "CNN_TBB" : 0,
		"CNN_TX" : 1,
		"CNN_TY" : 1,
		"CNN_TXX": 0,
		"CNN_TYY": 0,
		"CNN_TKX": 1,
		"CNN_TKY": 1,
        
        # FC (全连接层) 相关参数
        "FC_TI"  : ti ,
        "FC_TN"  : tn ,
        "FC_TB"  : tb ,
        "FC_TNN" : 0 ,
        "FC_TII" : 0 ,
        "FC_TBB" : 0 ,
		"FC_LOOP_ORDER" :lp ,#["B", "N", "I"],#["B","N","I"],#["B","BB","N","I","II","NN"],
        
        # AP (平均池化层) 相关参数
        "AP_TN" :  1, 
        "AP_TX" :  1, 
        "AP_TY" :  1, 
        "AP_TNN" : 0,
        "AP_TXX" : 0,
        "AP_TYY" : 0,
        # MX 和 MP 相关参数
        "MP_TN" :  1, 
        "MP_TX" :  1, 
        "MP_TY" :  1, 
        "MP_TNN" : 0,
        "MP_TXX" : 0,
        "MP_TYY" : 0,
				
        # ELT (Element-wise 操作) 相关参数
        "ELT_TN" : 8, 
        
        # ACT (激活函数) 相关参数
        "ACT_TN" : 8,
        
        # SOFTMAX 相关参数
        "SOFTMAX_TN" : 8,
	}
	
	config.update({
        # memory (by bits)
        "WEI_PREC": 8,#(todos) make editable
        "ACT_PREC": 8,
        
        "DRAM_LEN": 512, #bits 
        
        "L2_LEN" : 64*8, #bits
        
        "L1_WEI_LEN" : config["FC_TI"]*config["FC_TN"]*8, #bits
        "L1_ACT_LEN" : config["FC_TI"]*config["FC_TB"]*8, #bits


		"multiplier2_radix": radix,
		
		
		"clock": 1,
		"cap_load": 0.1,
		"tech": "tsmc40",
	})
	
	sa = SimpleArch(config, model_name = model.__class__.__name__.lower(), 
		np_save = 1, 
		run_cpp = 1, 

		RUN_PE=1,
		RUN_WEI_BUFFERS=1,
		RUN_ACT_BUFFERS=1,
		RUN_ADDERS=0,

		RUN_MODEL =1,
		RUN_GOLDEN=1,
		RUN_L1 = 1,
		RUN_L2 = 1,
	
		SIM_CYCLES = 20, Randomize = True,
		EDAVerification = True,#False,
		Wei_Sparse = ws, Act_Sparse = acts ) #0 more sparse, 1 less sparse
	sa.infer_network(intermediate)
	
	#4. --> predict the powers using "chisel" vs. "our model"
	#sa.get_golden(intermediate)
	
	#5. plot accuracy
	#sa.plot_model()
	
	
