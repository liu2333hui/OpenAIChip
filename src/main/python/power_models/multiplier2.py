import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import LinearSVR, SVR
from sklearn.neural_network import MLPRegressor
import warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn.utils.validation")
#script generated by AI deepseek
import pickle
def log1p_inverse(x):
	return np.exp(x) - 1
		
if __name__ == "__main__2":
	def ones_count(x):
		"""计算数字 x 的二进制表示中 1 的个数"""
		return bin(x).count('1')

	def calculate_toggle(in_0, in_1, radix, prec1):
		# 计算 c_i 的值
		c = []
		mask = (1 << radix) - 1  # 生成掩码，如 radix=2 时 mask=0b11
		for i in range(0, prec1, radix):
			#print(mask)
			c_i = in_1 * (in_0 & mask)
			#print(mask, c_i)
			c.append(c_i)
			mask = (mask << radix) | ((1 << radix) - 1)  # 更新掩码
			# print(mask)
			
		# 计算 toggle
		toggle = 0
		toggle += ones_count(c[0] ^ 0)  # c0 ^ 0
		for i in range(1, len(c)):
			# print(c[i],toggle)
			toggle += ones_count(c[i-1] ^ c[i])  # c_{i-1} ^ c_i
		toggle += ones_count(c[-1] ^ c[0])  # c_n ^ 0

		return toggle

	# 示例数据
	in_0 = 1#0b11010101  # 示例输入
	in_1 = 63  # 示例输入
	radix = 1          # radix 值
	prec1 = 8          # prec1 值

	for in_0 in [0, 1, 3, 7, 15, 31, 63, 127, 255]:
	    # 计算 toggle
	    toggle = calculate_toggle(in_1, in_0, radix, prec1)
	    print(f"{in_0} Toggle: {toggle}")
def log1p_inverse(x):
    return np.exp(x) - 1
			

if __name__ == "__main__":

	def ones_count(x):
		"""计算数字 x 的二进制表示中 1 的个数"""
		return bin(x).count('1')

	# 加载数据
	df = pd.read_csv("generated/Multiplier2/tsmc40_dataset.txt",  delimiter="\t")
	print(df)
	# 计算 toggle（假设 in_0 是字符串格式，例如 "0,3"）
	def calculate_toggle(in_0_str):
		# 分割字符串并转换为整数
		bits = list(map(int, in_0_str.split(',')))
		if(bits.length == 1):
			return 0
		else:
			# 计算异或值（XOR）
			xor_result = bits[0] ^ bits[1]
			# print(bin(xor_result), bin(xor_result).count('1'))
			# 统计二进制中 1 的个数
			return bin(xor_result).count('1')

	def calculate_bits(in_0_str):
		return bin(int(in_0_str)).count("1")
	def calculate_toggle(row):
	    in_0 = int(str(row['in_0']))  # 将二进制字符串转换为整数
	    in_1 = int(str(row['in_1']))  # 将二进制字符串转换为整数
	    radix = int(np.log2(row['radix']))
	    prec1 = row['prec1']
	
	    # 计算 c_i 的值
	    c = []
	    mask = (1 << radix) - 1  # 生成掩码，如 radix=2 时 mask=0b11
	    for i in range(0, prec1, radix):
	        c_i = in_1 * (in_0 & mask)
	        c.append(c_i)
	        mask = (mask << radix) | ((1 << radix) - 1)  # 更新掩码
	
	    # 计算 toggle
	    toggle = 0
	    toggle += ones_count(c[0] ^ 0)  # c0 ^ 0
	    for i in range(1, len(c)):
	        toggle += ones_count(c[i-1] ^ c[i])  # c_{i-1} ^ c_i
	    toggle += ones_count(c[-1] ^ 0)  # c_n ^ 0
	
	    return toggle
		
	# 应用 toggle 计算
	# df['toggle_0'] = df['in_0'].apply(calculate_toggle)
	# df['toggle_1'] = df['in_1'].apply(calculate_toggle)
	# df['toggle_out'] = 
	df['bits_0'] = df['in_0'].apply(calculate_bits)
	df['bits_1'] = df['in_1'].apply(calculate_bits)
	# 添加新列 1/CLOCK（注意避免除以 0）
	df['inv_CLOCK'] = 1 / df['CLOCK'].replace(0, np.nan)  # 如果 CLOCK 为 0，替换为 NaN
	#df = df.dropna()  # 删除包含 NaN 的行
	
	# 过滤 multiplierType 为 HighRadixMultiplier 的行
	df_filtered = df[df['multiplierType'] == 'HighRadixMultiplier']
	
	# 计算 toggle 并添加到 DataFrame
	df['toggle'] = df.apply(calculate_toggle, axis=1)
	
	#in units
	df['Unit_Cycles'] = df['cycles']/10.0/df['N']
	#in Joules
	df['Energy'] = df['Total_Pwr'] * df['Unit_Cycles'] * df['CLOCK'] * 1e-9
	print(df[['Energy(nJ)', 'Unit_Cycles']])
	# 特征列
	# 2. 定义分组列
	group_cols = ['prec', 'buffered', 'fanout',  'terms', 
	             'CLOCK', 'cap_load', 'fanout_load','inv_CLOCK']

	features = ['fanout', 'inv_CLOCK', 'cap_load']
	
	
	# # 3. 定义回归函数
	# def fit_linear_regression(group):
	#     """对每个分组执行线性回归"""
	#     if len(group) < 1:  # 空分组处理
	#         return pd.Series({'coef': np.nan, 'intercept': np.nan})
	    
	#     try:
	#         # 创建并训练模型
	#         X = group[['toggle']].values.reshape(-1, 1)
	#         y = group['Total_Pwr'].values
	#         model = LinearRegression().fit(X, y)
	#         y_pred = model.predict(X)
	#         y_test = y
	#         mae = mean_absolute_error(y_test, y_pred)
	#         mse = mean_squared_error(y_test, y_pred)
	#         rmse = np.sqrt(mse)    
	#         # 计算相对误差
	#         relative_error = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
	            
				
	#         # 返回系数和截距
	#         return pd.Series({
	#             'coef': model.coef_[0],
	#             'intercept': model.intercept_,
	# 			        "MAE": mae,
	# 			        "MSE": mse,
	# 			        "RMSE": rmse,
	# 			        "RelErr": relative_error
				    
	#         })
	#     except Exception as e:
	#         print(f"Error in group {group.name}: {str(e)}")
	#         return pd.Series({'coef': np.nan, 'intercept': np.nan})

	# # 4. 执行分组回归
	# result_df = df.groupby(group_cols).apply(fit_linear_regression).reset_index()

	# # 5. 生成最终数据集
	# final_df = result_df[features+ ['coef', 'intercept', "MAE", "MSE", "RMSE", "RelErr"]]
	# print(final_df)
	# )

	# 目标变量
	# features = ['prec1','radix', 'inv_CLOCK', 'cap_load',#, #'toggle_0', 'toggle_1',
	# 'in_0', 'in_1']
	#, #'toggle_0', 'toggle_1',
	features = ['prec1','radix', 'inv_CLOCK', 'cap_load',] + \
	['bits_0', 'bits_1', 'in_0', 'in_1']
	#'in_0', 'in_1', 'bits_0', 'bits_1', 'toggle', 'prec1']
	out = 'Total_Pwr'#'Unit_Cycles'#
	df = df#df_filtered
	
	# features = ['fanout', 'inv_CLOCK', 'cap_load']
	# out = 'coef'
	# df = final_df
	
	y = df[out]*1e10
	X = df[features]*1e5
	X = np.log1p(X)
	y = np.log1p(y)	
		
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
	
	# X_train = X
	# X_test = X
	# y_train = y
	# y_test = y
	
	# 6. 定义模型列表
	models = {
	    "LinearRegression": LinearRegression(),
	    "DecisionTreeRegressor": DecisionTreeRegressor(random_state=42),
	    "RandomForestRegressor": RandomForestRegressor(random_state=42),
	    "GradientBoostingRegressor": GradientBoostingRegressor(random_state=42),
	    "LinearSVR": LinearSVR(random_state=42),
	    "SVR": SVR(),
	    "MLPRegressor": MLPRegressor(random_state=42, max_iter=1000)
	}
	
	# 4. 遍历模型，训练并评估
	results = {}
	
	trained_models = {}

	for name, model in models.items():
	    # 训练模型
	    model.fit(X_train, y_train)
	    
	    # 预测
	    # print(X_test)
	    # exit(0)
	    y_pred = model.predict(X_test)
	    
	    # 计算评估指标
	    mae = mean_absolute_error(y_test, y_pred)
	    mse = mean_squared_error(y_test, y_pred)
	    rmse = np.sqrt(mse)
	    
	    # 计算相对误差

			
	    relative_error = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
		
	    rel_err_inverse = np.mean(np.abs((log1p_inverse(y_test) - log1p_inverse(y_pred)) / log1p_inverse(y_test))) * 100
	    # print("data")
	    # print(log1p_inverse(y_test))
	    # print(log1p_inverse(y_pred))
	    # 保存结果
	    results[name] = {
	        "MAE": mae,
	        "MSE": mse,
	        "RMSE": rmse,
	        "Relative Error (%)": relative_error,
			"rel_err_inverse":rel_err_inverse
	    }
	    trained_models[name] = model
	    # 打印结果
	    print(f"Model: {name}")
	    print(f"  MAE: {mae}")
	    print(f"  MSE: {mse}")
	    print(f"  RMSE: {rmse}")
	    print(f"  Relative Error (%): {relative_error}")
	    print(f"  rel_err_inverse (%): {rel_err_inverse}")
	    print("-" * 30)
	
	# 5. 找到表现最好的模型
	best_model = min(results, key=lambda x: results[x]['RMSE'])
	print(f"\nBest Model: {best_model}")
	print(f"  RMSE: {results[best_model]['RMSE']}")
	print(f"  Relative Error (%): {results[best_model]['Relative Error (%)']}")
	print(f"  rel_err_inverse (%): {results[best_model]['rel_err_inverse']}")
	
	with open('generated/PowerModels/multiplier2.pkl', 'wb') as f:
	    pickle.dump(trained_models[best_model], f)
	
	
if __name__ == "__main__22":
	with open('generated/PowerModels/multiplier2.pkl', 'rb') as f:
	    best_m = pickle.load(f)
		
		
	features = ['prec1','radix', 'inv_CLOCK', 'cap_load',#, #'toggle_0', 'toggle_1',
		'in_0', 'in_1']
				
	# toggle vs. power
	data = [[8,2, 1/1, 0.1  ,2 , 92], 
			[8,2, 1/1, 0.1  ,3 , 233], 
			[8,2, 1/1, 0.1  ,4 , 3],
			[8,2, 1/1, 0.1  ,20, 40],
			[8,2, 1/1, 0.1  ,3 , 3]
			]
	# features = ['prec1','radix', 'inv_CLOCK', 'cap_load',#, #'toggle_0', 'toggle_1',
	# 'in_0', 'in_1']
	testing_df = pd.DataFrame(data, columns=features)#['fanout', 'inv_CLOCK', 'cap_load','toggle'])
	X = np.log1p(testing_df[features]*1e5)
	
	# best_m = trained_models["SVR"]
	y =  log1p_inverse(best_m.predict(X))/1e10
	
	print(y)
	# import matplotlib.pyplot as plt
	# plt.plot([1,1.5,2,2.5,3], y )
	# plt.show()
	