import os
#new_path = "/nfs/project/JayMok/power_experiments_xie/primitives"
#os.environ['LD_LIBRARY_PATH'] = os.pathsep.join([new_path, os.environ.get('LD_LIBRARY_PATH', '')])
#print(os.environ['LD_LIBRARY_PATH'])
IS_LINUX = 1
SBT = "/afs/ee.ust.hk/staff/ee/jaymok/.local/share/coursier/bin/sbt"
import sys
#power_models_dir = os.path.dirname(os.path.abspath(__file__))
#sys.path.append(power_models_dir)
#power_models_dir = os.path.dirname(os.path.abspath(__file__) + "/power_models")
#sys.path.append(power_models_dir)


if(IS_LINUX):
	import ctypes
	ctypes.CDLL('/nfs/project/JayMok/power_experiments_xie/primitives/libstdc++.so.6')
	ctypes.cdll.LoadLibrary('/nfs/project/JayMok/power_experiments_xie/primitives/libstdc++.so.6')

#a hack
from power_models.AccumulatorPrimitive import AccumulatorPrimitive
from power_models.AdderNPrimitive import AdderNPrimitive
from power_models.Multiplier2Primitive import Multiplier2Primitive
from power_models.ConstantMultiplierPrimitive import ConstantMultiplierPrimitive
	
	
#A simple model, where each operation is a seperate module
#Spatial Architecture
from abc import ABC, abstractmethod

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn.utils.validation")
#script generated by AI deepseek
import paddle.nn as nn

import os
import json

#Helpers
def log1p_inverse(x):
	return np.exp(x) - 1
		

def trace_file(num, TRACE_FILE, input_data, r = 1):
	
	for t in range(num):
		file_path = f"{TRACE_FILE}_{t}"#+str(t)
		result = []
		# 打开文件进行读取
		with open(file_path, 'r') as file:
			# 读取所有行
			lines = file.readlines()
			
			# 遍历每一行
			for line in lines:
				# 按空格分割每行数据
				row_data = [int(num) for num in line.strip().split()]
				# 若结果列表为空，先初始化结果列表
				if not result:
					result = [[] for _ in range(len(row_data))]
				# 将每行数据添加到对应列的列表中
				for i, num in enumerate(row_data):
					result[i].append(num)
		input_data[f"in_{t}"] = [r*re for re in result]
	return input_data
	

def bit_count_helper():
	return """
	// 计算单个数字的1的数量
	int countBits(int num) {
	    int count = 0;
	    while (num) {
	        num &= (num - 1); // 清除最低位的1
	        count++;
	    }
	    return count;
	}
	
	// 对整个数组进行计算，并保存到新数组
	int* countBitsInArray(int* arr, int size) {
	    // 创建一个新数组，用于存储每个数字的1的数量
	    int* bitCounts = new int[size];
	
	    // 遍历原始数组，计算每个数字的1的数量
	    for (int i = 0; i < size; i++) {
	        bitCounts[i] = countBits(arr[i]);
	    }
	
	    return bitCounts; // 返回新数组
	}
	"""

def cpp_read_file_helper():
    return '''
	#include <iostream>
	#include <fstream>
	#include <cstdlib>  // Pour malloc et free

	// Fonction pour lire un fichier et stocker les valeurs dans un tableau
	int* lireFichierEtRemplirTableau(const char* filename, int*size, int count) {
		std::ifstream file(filename);
		if (!file.is_open()) {
			std::cerr << "Erreur : Impossible d'ouvrir le fichier." << std::endl;
			return nullptr;
		}

		int count2 = 0;
		std::string line;
		while (std::getline(file, line)) {
			count2++;
		}

		file.clear();  // Effacer les flags d'erreur
		file.seekg(0); // Retourner au début du fichier

		int* array = (int*)malloc(count * sizeof(int));
		if (array == nullptr) {
			std::cerr << "Erreur : chec de l'allocation mémoire." << std::endl;
			file.close();
			return nullptr;
		}

		// Initialiser le tableau avec des zéros
		for (int i = 0; i < count; i++) {
			array[i] = 0;
		}

		int index = 0;
		while (std::getline(file, line)) {
			array[index] = std::stoi(line); // Convertir la ligne en entier
			index++;
		}

		file.close();

		*size = count2;

		return array;
	}
	'''

#root, the root to write
#name, the nameo f the layer
#PARAMETERS, dict of constants
#DATA, the input data, weights, outputs etc.
#NET_DATA, the NETS to monitor
def generate_cpp(root,name,PARAMETERS,DATA,NETS, loop_order, valid_loops, tiling_pre = "CNN_T",hardware_config = {}, SIM_CYCLE_LIM = -1,hooks = {
				"core_before_outer_group_start": "",
				"core_before_inner_group_start": "",
				"core_after_inner_group_start":"",
				"core_after_data_fetch":"",
				"core_after_inner_group_end":"",
				"core_after_outer_group_end": ""
				}, run_it=True
):
	PRE = tiling_pre
	if True:
		NET_DATA = NETS
		#1. define files
		CPP_FILE = root + "/"+name+".cpp"
		for n in NET_DATA:
			#TOGGLING_FILE is the toggling data
			#TRACE_FILE is the actual data
			#JSON file is the json used by scala
			#POWER GOLDEN FILE is the golden ptpx
			#RUNTIME file is related with the timing
			#(todos) AREA file is related with the area
			NET_DATA[n]['TOGGLING_FILE'] = root + "/"+name+"."+n+".toggling"
			NET_DATA[n]['TRACE_FILE'] = root + "/"+name+"."+n+".trace"
			NET_DATA[n]['JSON_FILE'] = root + "/" + name + ".json"
			NET_DATA[n]['POWER_GOLDEN_FILE'] = root + "/" + name + ".golden"	
			NET_DATA[n]['RUNTIME_FILE'] = root + "/" + name + ".runtime"	

		with open(CPP_FILE, "w") as f:
			f.write("#include <iostream>\n")
			f.write("#include <algorithm>\n")
			f.write(cpp_read_file_helper())
			f.write(bit_count_helper())
			f.write("int main(){\n")


			for p,v in PARAMETERS.items():
				f.write(f"int {p} = {v};\n")


			for i in DATA:
				name = i['name']
				filename = i['file']
				size = i['size']
				f.write(f"int {name}_size;\n")
				f.write(f"int* {name} = lireFichierEtRemplirTableau(\"{filename}\", &{name}_size, {size});\n") 	


			for nk in NET_DATA:
				n = NET_DATA[nk]
				net =nk# n["net"]
				units = n['units']
				prec = n['bins']
				for m in n['metadata']:
					
					f.write(f'''
					int {m}_{net}[{units}][{prec}];
					for (int k = 0; k < {units}; k++)
						for (int j = 0; j < {prec}; j++) {{
							{m}_{net}[k][j] = 0;
					}}''')

					f.write(f'''
					int prev_{net}[{units}];
					for(int k = 0; k < {units}; k++)
						prev_{net}[k] = 0;
				''')

					f.write(f'std::cout << "// Analyzing Workload" << std::endl;\n')
	
			# neck
			if True:
				#body 
				f.write(f'std::cout << "// Analyzing Workload" << std::endl;\n')


			for n in NET_DATA:
				net = n#n["net"]
				n = NET_DATA[n]
				group = n['group']
				TRACE_FILE = n['TRACE_FILE']#'#root+"/"+name+f".{net}.trace"
				for i in range(group):#terms = FC_TI for adder tree	
					f.write(f'std::ofstream goldenOutFile_{net}_{i}("{TRACE_FILE}_{i}");\n');

			#3.5
			#variable init	
			f.write(f'int sim_cycles = -1;\n')	
			for n in NET_DATA:
				net = n#["net"].lower()
				f.write(f'int {net} = 0;\n')

			#3.6
			#body
			#outer loop
			START = {}
			DEPTH = {}
			for v in valid_loops:
				START[v] = "0"
				DEPTH[v] = valid_loops[v]["LIM"]

			cnt = 0
			c = hardware_config
			for lp in loop_order:
				for v in valid_loops:
					if(lp[0] == v and c[PRE+lp] != 0):
						f.write(f'''int prev_{v} = 0;\n''')

			for lp in loop_order:
				for v in valid_loops:
					if(lp[0] == v and c[PRE+lp] != 0):
						start = START[lp[0]]
						depth = DEPTH[lp[0]]
						f.write(f"for(int {lp} = {start}; {lp} < {start+'+'+depth}; \
						{lp} += {c[PRE+lp]})"+"{\n")
						DEPTH[lp[0]] = PRE+lp
						START[lp[0]] = lp
						cnt += 1
		
			#self.outer_cnt = cnt
			#3.7
			#body
			#inner loop
		
			INNER_LOOPS = {}
			inner_cnt = 0
			for v in valid_loops:
				#variable = v[1:].upper() #TB, the B here
				variable = v
				lowv = variable.lower()
				lim = valid_loops[variable]["LIM"]
				INNER_LOOPS[variable] = f'''for (int {lowv} = {variable}; {lowv} < std::min({variable} + {c[PRE+variable]}, {lim}); {lowv}++){{'''
				inner_cnt += 1
		
			print(INNER_LOOPS)	
			#3.8
			#interior core
			#dantian
			#(this part is the custom part that one should write, is functional too)
			CYCLE_LIMIT = SIM_CYCLE_LIM
			f.write(f'''
				sim_cycles++;
				if(sim_cycles > {CYCLE_LIMIT})
					break;
				''')
		
			
			f.write(hooks['core_before_outer_group_start'])
		
			
			outer_loops = 0
			for v in valid_loops:
				group = valid_loops[v]['GROUP']
				if(group != "INNER"):
					#print(v)
					#print(inner_tiles)
					#print(inner_tiles[v])
					#print(INNER_LOOPS[inner_tiles[v]])
					f.write(INNER_LOOPS[v])
					outer_loops += 1
		
			f.write(hooks['core_before_inner_group_start'])
		
			for n in NET_DATA:
				net = n.lower()#net = n["net"].lower()
				n = NET_DATA[n]	
				group = n['group']
				f.write("int accumulate_{net} = 0;\n")
					
			inner_loops = 0
			for v in valid_loops:
				group = valid_loops[v]['GROUP']
				if(group == "INNER"):
					f.write(INNER_LOOPS[v])
					inner_loops += 1
		
			f.write(hooks['core_after_inner_group_start'])
		
			for d in DATA:
				name = d['name']
				size = d['size']
				indexing = d['indexing']
				f.write(f'''
				int {name}_idx = {indexing};
				int {name}_data = {name}[{name}_idx];
				''')
				
			f.write(hooks['core_after_data_fetch'])
					
			#update toggle bins and previous values
		
			for n in NET_DATA:
				net = n.lower()
				n = NET_DATA[n]
				f.write('int {net} = 0;\n')
		
		
				#"in_function": "weight_data * input_data",
				#"post_function": "",#after accumulation
				#"accumulate": True
				update = n['update']
		
				units = n['units']
				accumulate = n['accumulate']
	
	
				f.write(f'''
				if(sim_cycles > 0)
					int update = {update}; 
					toggle_bins[{units}][__builtin_popcount(update^prev_{net}[{net}])]++; 
						prev_{net}[{net}] = {update};
				''')
						
				if(accumulate):
					f.write(f"accumulate_{net} += prev_{net}[{net}]")
						
				f.write(f'''
				{net} = ({net}+1);
				''')
		
			for i in range(inner_loops):
				f.write(f'}}')
		
			f.write(hooks['core_after_inner_group_end'])
		
			for i in range(outer_loops):
				f.write(f'}}')
		
			f.write(hooks['core_after_outer_group_end'])


			#tail 
			for i in range(cnt):
				f.write("}\n")
				
			#SAVE RESULTS
			f.write(f'std::cout << "// Saving Data" << std::endl;\n')
			#f.write(f'''
			#	std::ofstream timeFile("{RUNTIME}");
			#	timeFile << sim_cycles << "\\n";
			#	timeFile.close();''')

			for n in NET_DATA:
				net = n#n["net"]
				n = NET_DATA[n]
				units = n['units']
				bins = n['bins']
	
				group = n['group']
				TOGGLING_FILE = n['TOGGLING_FILE']#'#root+"/"+name+f".{net}.trace"
				
				units = n['units']
				prec = n['bins']
				for m in n['metadata']:
					
					f.write(f'std::ofstream ToggleOutFile_{m}_{net}("{TOGGLING_FILE}");\n');
	
					f.write(f'''
					int {m}_{net}[{units}][{prec}];
					for (int k = 0; k < {units}; k++)
						for (int j = 0; j < {prec}; j++) {{
					ToggleOutFile_{m}_{net}	<<	{m}_{net}[k][j] << "\n" ;
					}}''')

					f.write(f'ToggleOutFile_{m}_{net}.close();\n');
	

			for n in NET_DATA:
				net = n#n["net"]
				n = NET_DATA[n]
				group = n['group']
				TRACE_FILE = n['TRACE_FILE']#'#root+"/"+name+f".{net}.trace"
				for i in range(group):#terms = FC_TI for adder tree	
					f.write(f'goldenOutFile_{net}_{i}.close();\n');
				
				f.write(f'ToggleOutFile_{net}_{i}.close();\n');


				#std::cout << "// Saving ADDER/ACCUM Data - DONE" << std::endl;	

			for d in DATA:
				name = d['name']
				size = d['size']
				indexing = d['indexing']
				f.write(f'''
					free({name});
					''')
	
			f.write(f'''return 0;
				}}''')
	


	
		"""
			f.write(f'''
					sim_cycles++;
					if(sim_cycles > {self.SIM_CYCLES})
						break;

					pe = 0;
					//(todos) technically, psum does not go to 0 immediately
					// It will depend on different dataflows ...
					// etc.   TN TI TB, clearly accumulator will accumulate
					// next cycle
					psum = 0;
			
						 {b_up}
						 {n_up}
						
						int sum = 0;
						 {i_up}	
						int w_idx = i*{OUT} + n;
						int i_idx = b*{BAT} + i;
						int wb = weight[w_idx];//weight_bits[w_idx];
						int ib = input[i_idx];//input_bits[i_idx];	
						
						if(sim_cycles > 0)						
						toggle_bins[pe][__builtin_popcount(wb^prev_val[pe])]++; 
						prev_val[pe] = wb*ib;
						sum += prev_val[pe];

					''')

			
				f.write(f'''
					pe = (pe+1);
					}}


					//(todos) may have to change, 1. not every cycle the same accumulator, 2. maybe there is resets as well
					int new_sum = prev_out[psum] + sum;
					if(sim_cycles > 0){{	
					toggle_psum[psum][__builtin_popcount(sum^prev_psum[psum])]++; 
					toggle_output[psum][__builtin_popcount(new_sum^prev_out[psum])]++; 


					}}

					prev_psum[psum] = sum;	
					prev_out[psum] = new_sum;
				''')



				f.write(f'''
				psum += 1;	
	
				{(len(inner_tiles)-1)*"}"}
				''')
				

				if(self.RUN_GOLDEN):
					for i in range(self.FC_TI):	
						for psum in range(self.FC_PES//self.FC_TI):
							pe = i + self.FC_TI*psum
							f.write(f'goldenOutFile_PE_{i} << prev_val[{pe}] <<"\t";\n')

	
					for i in range(1):
						for psum in range(self.FC_PES//self.FC_TI):
							f.write(f'goldenOutFile_OUT_{i} << prev_out[{psum}] <<"\t";\n')
							f.write(f'goldenOutFile_PSUM_{i} << prev_psum[{psum}] <<"\t";\n')
	

				if(self.RUN_GOLDEN):

					for i in range(self.FC_TI):	
						f.write(f'goldenOutFile_PE_{i} << "\\n";\n')	
					for i in range(1):	
						f.write(f'goldenOutFile_OUT_{i} << "\\n";\n')
						f.write(f'goldenOutFile_PSUM_{i} << "\\n";\n')

				
				for i in range(cnt):
					f.write("}\n")
				
				# tail
				#SAVE RESULTS
				# f.write(f'std::cout << "// Analyzing Workload - DONE" << std::endl;\n')
				# f.write(f'std::cout << "// Saving Data" << std::endl;\n')
				f.write(f'''
					std::ofstream timeFile("{RUNTIME}");
					timeFile << sim_cycles << "\\n";
					timeFile.close();					
	
					std::ofstream outFile_0("{PE_OUT_FILE}"); // 创建或覆盖文件
					std::ofstream outFile_2("{OUT_OUT_FILE}"); // 创建或覆盖文件
					std::ofstream outFile_1("{PSUM_OUT_FILE}"); // 创建或覆盖文件
	
				
					//save psum_toggles
					for (int i = 0; i < {self.FC_PES}; i++){{
						for (int j = 0; j < {prec}; j++) {{
							outFile_0 << toggle_bins[i][j] << "\\n";
						}}
					}}
					//save adder_N toggles
					for (int i = 0; i < {self.FC_PES//self.FC_TI}; i++){{
						for (int j = 0; j < {prec}; j++) {{
							outFile_1 << toggle_psum[i][j] << "\\n";
						}}
					}}
					//save output_toggles (todos)
					for (int i = 0; i < {self.FC_PES//self.FC_TI}; i++){{
						for (int j = 0; j < {prec}; j++) {{
							outFile_2 << toggle_output[i][j] << "\\n";
						}}
					}}
	

					''')

				if(self.RUN_GOLDEN):
					#f.write('goldenOutFile_OUT.close();\n')
					#f.write('goldenOutFile_PSUM.close();\n')
					#f.write('goldenOutFile_PE.close();\n')
					for i in range(self.FC_TI):#}; i++){{')		
						f.write(f'goldenOutFile_PE_{i}.close();\n');
					for i in range(1):
						f.write(f'goldenOutFile_PSUM_{i}.close();\n');
	
						f.write(f'goldenOutFile_OUT_{i}.close();\n');
	


					#f.write(f'for(int i = 0; i < {self.FC_PES//self.FC_TI}; i++){{')		
					#f.write(f'std::ofstream goldenOutFile_PSUM_{i}.close();\n');
					#f.write(f'std::ofstream goldenOutFile_PE_{i}.close();\n');
					#f.write(f'std::ofstream goldenOutFile_OUT_{i}.close();\n');


	
	
				f.write(f'''		
					
					outFile_0.close();
					outFile_1.close();
					outFile_2.close();



					//std::cout << "// Saving ADDER/ACCUM Data - DONE" << std::endl;	
					free(weight);
					free(input);
	
					return 0;
				}}''')
			
		if(True):
			if(self.run_cpp):	
				if(not IS_LINUX):
					os.system(f"g++ -O3 {TOGGLE_FILE}") #make this as fast as possible please	
					os.system(".\\a.exe")
				else:
					os.system(f"g++ -O3 {TOGGLE_FILE} -Wl,-rpath,/nfs/project/JayMok/power_experiments_xie/primitives")
					os.system("./a.out")
		
			#we should run the power of the golden as well, as well as (todos)
			#inference of the power estimated by our power model
					
			if(self.RUN_GOLDEN):
				#GOLDEN FILE FOR ADDER TREE
				CONFIG = {
						"EDAVerification": self.EDAVerification,
						"CLOCK": self.clock,
						"cap_load": self.cap_load,
						"fanout_load": 0.0,
						"tech": self.tech,
						
						"terms": self.FC_TI,#tile,
						"adderType": "AddTreeN",
						"prec_in": prec,
						"prec_sum": prec,	
						
						"OutputPowerFile": ADDER_TREE_POWER_GOLDEN_FILE				  
					}
				with open(JSON_FILE, "w") as json_file:
					json.dump(CONFIG, json_file, indent=4)  # indent 用于格式化输出	
	
					PE_TRACE_FILES = " ".join([PE_TRACE_FILE+"_"+str(i) for i in range(self.FC_TI)])
					print(self.FC_TI)
					print("run sbt")

				print(f'{SBT} "test:runMain adders.AdderNSpecFromFile {PE_TRACE_FILES} {JSON_FILE}"')# > golden.addern.log')


				os.system(f'{SBT} "test:runMain adders.AdderNSpecFromFile {PE_TRACE_FILES} {JSON_FILE}"')# > golden.addern.log')

				#GOLDEN FILE FOR ACCUMULATOR
				CONFIG = {
						"EDAVerification": self.EDAVerification,
						"CLOCK": self.clock,
						"cap_load": self.cap_load,
						"fanout_load": 0.0,
						"tech": self.tech,
						
						"terms": 1,#(todos), could be more tile,
						"prec_in": prec,
						"prec_sum": prec,	
						
						"OutputPowerFile": ACCUMULATOR_POWER_GOLDEN_FILE				  
					}
				with open(JSON_FILE, "w") as json_file:
					json.dump(CONFIG, json_file, indent=4)  # indent 用于格式化输出	
	
					ACCUM_TRACE_FILE = OUT_TRACE_FILE
					ACCUM_TRACE_FILES = " ".join([ACCUM_TRACE_FILE+"_"+str(i) for i in range(1)])
				os.system(f'{SBT} "test:runMain accumulators.RegAccumulatorNSpecFromFile {ACCUM_TRACE_FILES} {JSON_FILE}"')# > golden.accum.log')



					#GOLDEN_FILE for Accumulator
					#with open(POWER_GOLDEN_FILE) as f:
					#	for l in f.readlines():
					#		print(l)
					
				
			#ADDER TREE:
			at_estimate_pwr = 0	
			at_estimate_runtime = 0
			at_estimate_energy = 0

			at_baseline1_pwr = 0	
			at_baseline1_runtime = 0
			at_baseline1_energy = 0

			at_baseline2_pwr = 0	
			at_baseline2_runtime = 0
			at_baseline2_energy = 0

			at_golden_pwr = 0
			at_golden_runtime = 0
			at_golden_energy = 0
	
			acc_estimate_pwr = 0	
			acc_estimate_runtime = 0
			acc_estimate_energy = 0

			acc_baseline1_pwr = 0	
			acc_baseline1_runtime = 0
			acc_baseline1_energy = 0

			acc_baseline2_pwr = 0	
			acc_baseline2_runtime = 0
			acc_baseline2_energy = 0

			acc_golden_pwr = 0
			acc_golden_runtime = 0
			acc_golden_energy = 0
	

			#with open(RUNTIME, 'r') as f:
			#	estimate_runtime = int(f.readlines()[0].strip())	
			if(self.RUN_MODEL):
				#Get power of the Adder Tree
				with open( PE_OUT_FILE , "r") as f:
					toggle_bins = np.zeros((self.FC_PES*prec))#custom in the future;
					for idx,l in enumerate(f.readlines()):
						toggle_bins[idx] = int(l)
					toggle_bins = toggle_bins.reshape((self.FC_PES//self.FC_TI, self.FC_TI, prec))#.reshape((-1))
					
					
					from power_models.AdderNPrimitive import AdderNPrimitive
					ap = AdderNPrimitive()
					# ap.execute_get_lut(out_features=["Total_Pwr"], constant_features = {
					# 	"CLOCK": self.clock,
					# 	"cap_load": self.cap_load,
					# 	"prec_in": prec,
					# 	"prec_sum": prec,
					# 	"terms": self.FC_TI
					# },
					# variable_features = {

					# })
					#(just do direct inference for now)
					#input_data, simply read it from the 
					# 初始化一个空列表用于存储最终结果
					r = 5
					input_data = {
							"CLOCK": [self.clock],
							"cap_load": [self.cap_load],
							"prec_in" : [prec],
							"prec_sum": [prec],
							"terms": [self.FC_TI]
						}

					input_data = trace_file(num=self.FC_TI, TRACE_FILE=PE_TRACE_FILE, input_data=input_data, r = r)


					#print(input_data)
					res = ap.execute_testing(
						name = "AdderN",
						out_features = ['Total_Pwr'],#,'Unit_Cycles', 'Energy'],
						input_data = input_data
					)
					#print(res)
					at_estimate_pwr = res['Total_Pwr']['res_sum']
					
					power_per_toggle, avg_pwr = ap.execute_get_lut(name="AdderN",out_features="Total_Pwr", constant_features = {
					 	"CLOCK": self.clock,
					 	"cap_load": self.cap_load,
					 	"prec_in": prec,
					 	"prec_sum": prec,
					 	"terms": self.FC_TI
					},
						variable_features = {
						"sum_toggles_in": [i for i in range(prec*self.FC_TI)] ,
						"toggles_out_0": [i for i in range(prec)]
					})


					at_baseline1_pwr = self.FC_PES//self.FC_TI * avg_pwr
					at_baseline2_pwr = at_baseline1_pwr#assume same for now

					print("BASELINE1 ADDERTREE", at_baseline1_pwr)
					print("ESTIMATE ADDERTREE", at_estimate_pwr)
	
					if(self.RUN_GOLDEN):
					 	gold = pd.read_csv(ADDER_TREE_POWER_GOLDEN_FILE,  delimiter="\t")
					 	#get relevant rows
					 	rel = gold.tail(n=self.FC_PES//self.FC_TI)['Total_Pwr']
					 	print(rel, np.sum(rel))
					 	print("GOLDEN ADDER TREE", np.sum(rel))
					 	at_golden_pwr = np.sum(rel)
					

				#Get power of the Accumulator

				with open( OUT_OUT_FILE , "r") as f:
					acc = AccumulatorPrimitive()
					#acc.load("Accumulator1", ["Total_Pwr"] )
	
					#(ad-hoc)
					prec = 32
					r = 1
					input_data = {
							"CLOCK": [self.clock],
							"cap_load": [self.cap_load],
							"prec_in" : [prec],
							"prec_out": [prec],
							"terms": [1]
						}

					input_data = trace_file(num=1, TRACE_FILE=OUT_TRACE_FILE, input_data=input_data, r = 1)


					#print(input_data)
					#(please debug, why cannot load the pickle?)
					res = acc.execute_testing(
						name = "Accumulator1",
						out_features = ['Total_Pwr'],#,'Unit_Cycles', 'Energy'],
						input_data = input_data
					)
					#print(res)
					acc_estimate_pwr = res['Total_Pwr']['res_sum']
					
					power_per_toggle, avg_pwr = acc.execute_get_lut(name="Accumulator1",out_features="Total_Pwr", constant_features = {
					 	"CLOCK": self.clock,
					 	"cap_load": self.cap_load,
					 	"prec_in": prec,
					 	"prec_out": prec,
					 	"terms": 1
					},
						variable_features = {
						"toggles_in_0": [i for i in range(prec)] ,
						"toggles_out_0": [i for i in range(prec)]
					})
					acc_baseline1_pwr = self.FC_PES//self.FC_TI * avg_pwr
					acc_baseline2_pwr = acc_baseline1_pwr#assume same for now
					print("BASELINE1 ACCUM", acc_baseline1_pwr)
					print("ESTIMATE ACCUM", acc_estimate_pwr)
					if(self.RUN_GOLDEN):
					 	gold = pd.read_csv(ACCUMULATOR_POWER_GOLDEN_FILE,  delimiter="\t")
					 	#get relevant rows
					 	rel = gold.tail(n=self.FC_PES//self.FC_TI)['Total_Pwr']
					 	print(rel, np.sum(rel))
					 	print("GOLDEN ACCUM", np.sum(rel))
					 	acc_golden_pwr = np.sum(rel)
					



			results = []

	
			res = {}
			res['name'] = "ADDER_TREE"

			res['estimate_pwr'] = at_estimate_pwr 
			res['estimate_runtime'] = at_estimate_runtime
			res['estimate_energy'] = at_estimate_energy

			res['baseline1_pwr'] = at_baseline1_pwr 
			res['baseline1_runtime'] = at_baseline1_runtime
			res['baseline1_energy'] = at_baseline1_energy

			res['baseline2_pwr'] = at_baseline2_pwr 
			res['baseline2_runtime'] = at_baseline2_runtime
			res['baseline2_energy'] = at_baseline2_energy	

			res['golden_pwr'] = at_golden_pwr
			res['golden_runtime'] = at_golden_runtime
			res['golden_energy'] = at_golden_energy

			results.append(res)

			res = {}
			res['name'] = "ACCUMULATOR"

			res['estimate_pwr'] = acc_estimate_pwr 
			res['estimate_runtime'] = acc_estimate_runtime
			res['estimate_energy'] = acc_estimate_energy

			res['baseline1_pwr'] = acc_baseline1_pwr 
			res['baseline1_runtime'] = acc_baseline1_runtime
			res['baseline1_energy'] = acc_baseline1_energy

			res['baseline2_pwr'] = acc_baseline2_pwr 
			res['baseline2_runtime'] = acc_baseline2_runtime
			res['baseline2_energy'] = acc_baseline2_energy	

			res['golden_pwr'] = acc_golden_pwr
			res['golden_runtime'] = acc_golden_runtime
			res['golden_energy'] = acc_golden_energy

			results.append(res)	
			#exit()
			return results
	

		"""	


#Estimate (our model using binning, toggling analysis, state-based)
#Baseline1, using average power (i.e. NeuroSim-like)
#Baseline2, using average energy (i.e. Accelergy-like)

class WinogradArch:#(AIChip):
	"""子类，实现具体的推理功能。"""

	def __init__(self, config, model_name = "Testing", run_cpp = True, np_save = True,
		RUN_PE = True, RUN_WEI_BUFFERS = True, RUN_ACT_BUFFERS = True,
		RUN_MODEL = True,
		RUN_GOLDEN=True,
		SIM_CYCLES = 88888888888888, #some gigantic number
		Randomize = False, Wei_Sparse = 0.5, Act_Sparse = 0.5,
		EDAVerification = False,

		RUN_MAPPER = True,
		RUN_L1 = True,
		RUN_L2 = True,
		RUN_ADDERS = True,

		logs = "logs2",
		
		):


		self.multiplier2_radix = config['multiplier2_radix']

		self.clock = config['clock']
		self.cap_load = config['cap_load']
		self.tech = config['tech']
	
		self.RUN_PE = RUN_PE
		self.RUN_WEI_BUFFERS = RUN_WEI_BUFFERS
		self.RUN_ACT_BUFFERS = RUN_ACT_BUFFERS
		self.RUN_L1 = RUN_L1
		self.RUN_L2 = RUN_L2
		self.RUN_ADDERS = RUN_ADDERS
		self.RUN_MAPPER = RUN_MAPPER
		
		self.RUN_GOLDEN = RUN_GOLDEN
		self.Wei_Sparse = Wei_Sparse
		self.Act_Sparse = Act_Sparse
	
		self.RUN_MODEL = RUN_MODEL
		self.np_save = np_save
		self.run_cpp = run_cpp
		self.SIM_CYCLES = SIM_CYCLES
		self.Randomize = Randomize
		self.EDAVerification = EDAVerification
		
		# CNN 相关参数
		self.CNN_TI = config["CNN_TI"]
		self.CNN_TN = config["CNN_TN"]
		self.CNN_TB = config["CNN_TB"]
		self.CNN_TNN = config["CNN_TNN"]
		self.CNN_TII = config["CNN_TII"]
		self.CNN_TBB = config["CNN_TBB"]

		self.CNN_TKX = config["CNN_TKX"]
		self.CNN_TKY = config["CNN_TKY"]
		self.CNN_TX = config["CNN_TX"]
		self.CNN_TY = config["CNN_TY"]	

		self.CNN_TXX = config["CNN_TXX"]
		self.CNN_TYY = config["CNN_TYY"]	
		
		self.CNN_LOOP_ORDER = config["CNN_LOOP_ORDER"]
		
		self.hardware_config = {
			"CNN_TI": self.CNN_TI,
			"CNN_TX": self.CNN_TX,
			"CNN_TY": self.CNN_TY,
			"CNN_TN": self.CNN_TN,
			"CNN_TKX": self.CNN_TKX,
			"CNN_TKY": self.CNN_TKY,
			"CNN_TB": self.CNN_TB,	
		}



		# memory (by bits)
		self.WEI_PREC = config["WEI_PREC"]
		self.ACT_PREC = config["ACT_PREC"]
		
		self.DRAM_LEN = config["DRAM_LEN"]#512 
		
		self.L2_LEN = config["L2_LEN"]#256
		
		self.L1_WEI_LEN = config["L1_WEI_LEN"]#256
		self.L1_ACT_LEN = config["L1_ACT_LEN"]# 256
		
		design = [self.CNN_TI,  self.CNN_TN,  self.CNN_TB,
				self.CNN_TII, self.CNN_TNN, self.CNN_TBB,
				self.CNN_TKX, self.CNN_TKY, self.CNN_TX,
				self.CNN_TY, 
				self.CNN_TXX, self.CNN_TYY, 
				self.DRAM_LEN, self.L2_LEN, self.L1_WEI_LEN, self.L1_ACT_LEN
				]
		design = "_".join([str(d) for d in design])
		bn = [
			SIM_CYCLES, 
			Randomize, Wei_Sparse, Act_Sparse
		]
		benchmark = "_".join([str(d) for d in bn])
		self.root = "generated/Architecture/WinogradArch/Design_"+design + "__" + model_name + "__" + benchmark
		self.log_root = f"generated/Architecture/WinogradArch/{logs}/Design_"+design + "__" + model_name + "__" + benchmark

		if not os.path.exists(f"generated/Architecture/WinogradArch/{logs}"):
		    os.mkdir(f"generated/Architecture/WinogradArch/{logs}")	

		if not os.path.exists(self.log_root):
		    os.mkdir(self.log_root)
	
		if not os.path.exists(self.root):
		    os.mkdir(self.root)
	
	def infer_cnn(self, name, input_data, weights,out_data, stride, padding, Randomize = False):
		
		kernel_size = [weights.shape[-2], weights.shape[-1]]
		stride = stride
		padding = padding

		X, Y, KX, KY, IN, OUT, BAT, w_file, i_file, weights, input_data = self.infer_cnn_prepare(name, input_data, weights, out_data,Randomize)
		
		#prepare winograd transform matrices
		GX,GY,AX,AY,BX,BY, g_file,gt_file,a_file,at_file,\
			b_file,bt_file, wino_G, wino_GT, wino_A, wino_AT,\
				wino_B, wino_BT = self.infer_winograd_prepare(name+".winograd")		


		#1. Create C++
		results = []

		if(self.RUN_MAPPER):
			mp_res = self.infer_winograd_mapper(name, X, Y, KX, KY, IN, OUT, BAT, w_file, i_file, weights, input_data,
GX,GY,AX,AY,BX,BY, g_file,gt_file,a_file,at_file,\
			b_file,bt_file, wino_G, wino_GT, wino_A, wino_AT,\
				wino_B, wino_BT)
			results = results + mp_res
	
		
		'''
		if(self.RUN_L1 or self.RUN_L2 or self.RUN_WEI_BUFFERS or self.RUN_ACT_BUFFERS):

			b_res = self.infer_fc_in_buffers(name,

			IN, OUT, BAT, w_file, i_file, weights, input_data, out_data)

			results = results + b_res
	
		if(self.RUN_PE):
			m_res = self.infer_fc_mult(name, IN, OUT, BAT, w_file, i_file, weights, input_data)
			results = results + m_res
	
	
		if(self.RUN_ADDERS):
			a_res = self.infer_fc_adder_accum(name, IN, OUT, BAT, w_file, i_file, weights, input_data)
			print(a_res)
			#exit()
			results = results +  a_res
		'''	
		exit()	
		#OUT (re-scaling)
		#OUT-L1
		#L1
		#L1-L2
		#L2
		#L2-DRAM
		
		return results	


		
		
		print(input_data[0].shape)
		print(weights.shape)
		exit(0)
		#return self.infer_fc(name, (img2col_input.reshape(img2col_input.shape).T,), img2col_weights.T,
		#	out_data, Randomize = Randomize)

	def infer_winograd_prepare(self, name):
		import wincnn
		wino_AT,wino_G,wino_BT,wino_f = wincnn.cookToomFilter([0,1,-1,2,-2, 3, -3, 4, -4, 5, -5, 6, -6, 7, -7, 8, -8, 9, -9, 10, -10],
                    self.CNN_TX, self.CNN_TKX)
		MU = max([gg.denominator for gg in np.array(wino_G).reshape(-1)])
		MU_A = max([gg.denominator for gg in np.array(wino_AT).reshape(-1)])
		wino_G = np.array(MU*wino_G).astype('int')
		wino_A = np.array(wino_AT).transpose()
		wino_GT = np.array(wino_G).transpose()
		wino_B = np.array(wino_BT).transpose()
		wino_BT = np.array(wino_BT)
		wino_AT = np.array(wino_AT)
		#For a tile of :
		#TI, TN, TB
		#TX, TY, TKX, TKY
		#np.matmul(np.matmul(AT,np.matmul(np.matmul(G,W),GT)*np.matmul(np.matmul(BT,I),B)),A)
		print(wino_G, wino_GT)
		print(wino_A, wino_AT)
		print(wino_B, wino_BT)
	
	
		#exit(0)
                      		


		g_file = self.root+"/"+name+".g.txt"
		gt_file = self.root+"/"+name+".gt.txt"
		a_file = self.root+"/"+name+".a.txt"
		at_file = self.root+"/"+name+".at.txt"
		b_file = self.root+"/"+name+".b.txt"
		bt_file = self.root+"/"+name+".bt.txt"
		if(self.np_save):
			print(g_file)
			np.savetxt(g_file, wino_G, fmt='%d', delimiter='\n')
			np.savetxt(gt_file, wino_GT, fmt='%d', delimiter='\n')
			np.savetxt(a_file,wino_A , fmt='%d', delimiter='\n')
			np.savetxt(at_file,wino_AT, fmt='%d', delimiter='\n')
			np.savetxt(b_file, wino_B, fmt='%d', delimiter='\n')
			np.savetxt(bt_file, wino_BT, fmt='%d', delimiter='\n')

			print(g_file)
	
		GX,GY = wino_G.shape
		AX,AY = wino_A.shape
		BX,BY = wino_B.shape
	
	
		return GX,GY,AX,AY,BX,BY, g_file,gt_file,a_file,at_file,\
			b_file,bt_file, wino_G, wino_GT, wino_A, wino_AT,\
				wino_B, wino_BT

	def infer_cnn_prepare(self, name, input_data, weights, out_data,Randomize):
		#input_data = input_data[0]
		#0. Prepare data
		input_data = input_data[0] #if is tuple
		OUT =  weights.shape[0]
		IN = weights.shape[1]
		KX = weights.shape[2]
		KY = weights.shape[3]
		BAT = input_data.shape[0]
		input_IN = input_data.shape[1]
		X = input_data.shape[2]
		Y = input_data.shape[3]
		print(input_data.shape)
		print(weights.shape)
		assert(input_data.shape[1] == weights.shape[1])
		
		#quantize (fixed point)
		#(todos) some algorithm to choose the scaling ?
		weights    = ((weights*256*256) %self.WEI_PREC).astype(np.int32)
		input_data = ((input_data*256*256) %self.ACT_PREC).astype(np.int32)
		
		#Randomize weights and inputs (TODOS)
		if(Randomize):
			n = IN*OUT*KX*KY
			k = int(n*self.Wei_Sparse)
			rand_wei = np.random.randint(0, 256, size=n)
			zero_indices = np.random.choice(n, k, replace=False)
			rand_wei[zero_indices] = 0
			
			n = IN*BAT*X*Y
			k = int(n*self.Act_Sparse)
			rand_act = np.random.randint(0, 256, size=n)
			zero_indices = np.random.choice(n, k, replace=False)
			rand_act[zero_indices] = 0
			
			weights = rand_wei.reshape((OUT, IN, KX, KY))
			input_data = rand_act.reshape((BAT, IN, X, Y))
		
		w_file = self.root+"/"+name+".weights.txt"
		i_file = self.root+"/"+name+".input.txt"
		if(self.np_save):
			weights = rand_wei.reshape((-1))#OUT, IN, KX, KY))
			input_data = rand_act.reshape((-1))#BAT, IN, X, Y))
	
			np.savetxt(w_file, weights, fmt='%d', delimiter='\n')
			np.savetxt(i_file, input_data, fmt='%d', delimiter='\n')

		return X, Y, KX, KY, IN, OUT, BAT, w_file, i_file, weights, input_data 
		#return IN, OUT, BAT, w_file, i_file, weights, input_data





	'''
		//RAW WEIGHT : [TKX, TKY, TI, TN]
		// G x WEI: [(TKX+TX-1)*TKX, TKY, TI, TN]
		// sum(G x WEI): [(TKX+TX-1), TKY, TI, TN]
		// sum(G x WEI) x GT: [(TKX+TX-1), (TKY+TY-1)*TKY, TI, TN]
		// sum(sum(G x WEI) x GT): [(TKX+TX-1), (TKY+TY-1), TI, TN]
		// WEI_PE: [(TKX+TX-1)^2, (TKY+TY-1), TI, TB] 

		// RAW ACT : [(TX+TKX-1), (TY+TKY-1), TI, TB]
		// B x WEI: [(TKX+TX-1)^2, (TKY+TY-1), TI, TB]
		// sum(B x WEI): [(TKX+TX-1)^2, (TKY+TY-1), TI, TB]
		// sum(B x WEI) x BT
		// sum(sum(B x WEI) x BT): 
	'''
	#the input mapper
	def infer_winograd_mapper(self,name, X, Y, KX, KY, IN, OUT, BAT, w_file, i_file, weights, input_data, \
GX,GY,AX,AY,BX,BY, g_file,gt_file,a_file,at_file,\
			b_file,bt_file, wino_G, wino_GT, wino_A, wino_AT,\
				wino_B, wino_BT):

		TYPE_NAME = "MAPPER"
		buffer_loop_order = self.CNN_LOOP_ORDER

		#0. axioms
		PARAMETERS = {
			"X":X,
			"Y":KX,
			"KX":KX,
			"KY":KY,
			"IN":IN,
			"OUT":OUT,
			"BAT":BAT,
			"GX":GX,
			"GY":GY,
			"AX":AX,
			"AY":AY,
			"BX":BX,
			"BY":BY
		}
		
		DATA = [
			{
			"name": "weights",
			"file": w_file,
			"size": len(weights.reshape((-1))),
			"indexing": "n*IN*KX*KY+i*KX*KY+kx*KY + ky"
			},
			{
				"name": "input_data",
				"file": i_file,
				"size": len(input_data.reshape((-1))),
				"indexing": "b*IN*X*Y+i*X*Y+x*Y+y"	
			},
			{
				"name": "WINO_G",
				"file": g_file,
				"size": len(wino_G.reshape((-1))),
				"indexing": "gx*GY+gy"	
			},
			{
				"name": "WINO_GT",
				"file": gt_file,
				"size": len(wino_GT.reshape((-1))),
				"indexing": "gy*GX+gx"	
			},
			{
				"name": "WINO_A",
				"file": a_file,
				"size": len(wino_A.reshape((-1))),
				"indexing": "ax*AY+ay"	
			},
			{
				"name": "WINO_AT",
				"file": at_file,
				"size": len(wino_AT.reshape((-1))),
				"indexing": "ay*AX+ax"	
			},
			{
				"name": "WINO_B",
				"file": b_file,
				"size": len(wino_B.reshape((-1))),
				"indexing": "bx*BY+by"	
			},
			{
				"name": "WINO_BT",
				"file": bt_file,
				"size": len(wino_BT.reshape((-1))),
				"indexing": "by*BX+bx"	
			},
		]

		NET_DATA = {
			"RAW_WEI": {
				"units": self.CNN_TI*self.CNN_TN*self.CNN_TKX*self.CNN_TKY,
				"bins":self.WEI_PREC, 
				"group": 1, 
				"metadata": ["toggle"], 
				"reset_trigger": [],
				"update": "weights",
				"accumulate": False,			
			},
			"G_WEI": {
				"units": self.CNN_TI*self.CNN_TN*(self.CNN_TKX+self.CNN_TX-1)*self.CNN_TKX*self.CNN_TKY,
				"bins": self.WEI_PREC*2,
				"group":self.CNN_TKX+self.CNN_TX-1,
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "weights*G_WEI",
				"accumulate": False,			
			},
			"G_WEI_SUM": {
				"units": self.CNN_TI*self.CNN_TN*(self.CNN_TKX+self.CNN_TX-1)*self.CNN_TKY,
				"bins": self.WEI_PREC*2,
				"group":1,
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "",
				"accumulate": False,			
			},
		"G_WEI_GT": {
				"units": self.CNN_TI*self.CNN_TN*(self.CNN_TKX+self.CNN_TX-1)*(self.CNN_TKY+self.CNN_TY-1)*self.CNN_TKY,
				"bins": self.WEI_PREC*2,
				"group":self.CNN_TKY+self.CNN_TY-1,
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "",
				"accumulate": False,			
			},
			"G_WEI_GT_SUM": {
				"units": self.CNN_TI*self.CNN_TN*(self.CNN_TKX+self.CNN_TX-1)*(self.CNN_TKY+self.CNN_TY-1),
				"bins": self.WEI_PREC*2,
				"group":1,
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "",
				"accumulate": False,			
			},	
		}

		NET_DATA.update({
			"RAW_ACT": {
				"units": self.CNN_TI*self.CNN_TB*(self.CNN_TX+self.CNN_TKX-1)*(self.CNN_TY+self.CNN_TKY-1),
				"bins":self.ACT_PREC, 
				"group": 1, 
				"metadata": ["toggle"], 
				"reset_trigger": [],
				"update": "",	
				"accumulate": False,		
			},
			"B_ACT": {
				"units": self.CNN_TI*self.CNN_TB*(self.CNN_TX+self.CNN_TKX-1)**2*(self.CNN_TY+self.CNN_TKY-1),
				"bins": self.ACT_PREC*2,
				"group":self.CNN_TKX+self.CNN_TX-1,
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "",	
				"accumulate": False,		
			},
			"B_ACT_SUM": {
				"units": self.CNN_TI*self.CNN_TN*(self.CNN_TKX+self.CNN_TX-1)*(self.CNN_TKY+self.CNN_TY-1),
				"bins": self.ACT_PREC*2,
				"group":1,
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "",		
				"accumulate": True,		
			},
			"B_ACT_BT": {
				"units": self.CNN_TI*self.CNN_TN*(self.CNN_TKX+self.CNN_TX-1)*(self.CNN_TKY+self.CNN_TY-1)**2,#kind of like number of this kind of net
				"bins": self.ACT_PREC*2,#kind of like net's range
				"group":self.CNN_TKY+self.CNN_TY-1,#kind of like number of inputs, terms, net's size
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "",		
				"accumulate": False,	
			},
			"B_ACT_BT_SUM": {
				"units": self.CNN_TI*self.CNN_TN*(self.CNN_TKX+self.CNN_TX-1)*(self.CNN_TKY+self.CNN_TY-1),
				"bins": self.ACT_PREC*2,
				"group":1,
				"metadata": ["toggle"],
				"reset_trigger": [],
				"update": "",		
				"accumulate": True,
			},	
		})

		loop_order = self.CNN_LOOP_ORDER#["B", "N", "I"]
		#use single letter to signify
		valid_loops = {
			"B": {
				"LIM": "BAT",
				"STRIDE": 1,
				"GROUP": "OUTER"
			},
			"I": {
				"LIM": "IN",
				"STRIDE": 1,
				"GROUP": "INNER", #meaning can be collected together in the sum
			},
			"N": {
				"LIM": "OUT",
				"STRIDE": 1,
				"GROUP": "OUTER"
			},
			"X": {
				"LIM": "X",
				"STRIDE": 1,
				"GROUP": "OUTER"
			},
			"Y": {
				"LIM": "Y",
				"STRIDE": 1,
				"GROUP": "OUTER"
			},
		}


		root = self.root
		name = name
		generate_cpp(root, name, PARAMETERS, DATA, NET_DATA, loop_order, valid_loops,"CNN_T", self.hardware_config)

		results =[]
		return results
	

		

	#this one uses our power models
	def infer_network(self,intermediate_outputs,skips=[]):
		for layer_name, layer, input_data, out_data in intermediate_outputs:
			skip = 0
			for s in skips:
				if(isinstance(layer, s)):
					skip = 1
			if(skip == 1):
				continue
			if isinstance(layer, nn.Conv2D):
				#print(layer._stride)
				#print(layer)
				if(layer._kernel_size[0] == self.CNN_TKX
					and
				   layer._stride[0] == 1
					and
				   layer._stride[1] == 1	
					and
				   layer._kernel_size[1] == self.CNN_TKY):
					print("winograd valid")
					print(layer)	
					params = {
					"in_channels": layer._in_channels,
					"out_channels": layer._out_channels,
					"kernel_size": layer._kernel_size,
					"stride": layer._stride,
					"padding": layer._padding
				}
					results = self.infer_cnn(layer_name, input_data, layer.weight, out_data,
					stride = layer._stride[0],
					padding = layer._padding,
					Randomize=self.Randomize)
							
					self.save_results(layer_name,results)


			#print(f"Layer: {layer_name}")

	def save_results(self,layer_name, results):

			

		head = []
		line = []
		baseline1_pwr = 0
		baseline2_pwr = 0
		estimate_pwr = 0
		golden_pwr = 0
	
		vals = ['baseline1_pwr', 'baseline2_pwr', 'estimate_pwr', 'golden_pwr']
		#(todos) baseline_runtime, energy, area
		accum = [0]*len(vals)
		for res in results:
			#baseline1_pwr += res['baseline1_pwr']	
			#baseline2_pwr += res['baseline2_pwr']	
			#estimate_pwr += res['estimate_pwr']	
			#golden_pwr += res['golden_pwr']	

			for idx in range(len(accum)):
				accum[idx] += res[vals[idx]]

			head += [res['name']+"_"+v   for v in vals ]
			line += [res[v]   for v in vals]
		
		head += ['total_'+v for v in vals]
		line += accum	


		with open(self.log_root+'/'+layer_name+'.txt','w') as f:
			f.write('\t'.join(head) + '\n')
			f.write('\t'.join([str(l) for l in line]) + '\n')
	
	
import benchmarks.vision.vision as vision
from paddle.vision.models import LeNet, alexnet,AlexNet
from paddle.vision.models import vgg16, resnet50, resnet18, resnet34, resnet101, resnet152


#WinogradArch
# Same Mapping + Hardware for all inference layers
#	meaning no multi-precision, reconfigurable, systolic, sparsity, winograd
#	but we can have bit-serial inference
#	Convolution is reduced to Fully-connected Layer, this can have toll on the energy for L1
# Pick and Choose Layers to Infer
# Generate Power, Energy, Runtime (TODOS) Delay (i.e. for Timing), Area

if __name__ == "__main__":
	
    #benchmarks
	#lenet
	#alexnet
	#resnet
	
	#synthetic tests
	#1. get benchmarks say vision
	#2. for each benchmark, iterate each layer
	#3. corresponding infer --> get traces

	#OPTIMIZATION
	#1. PREPARE
	#2. INFER POWER

	images = ["src/test/resources/gou.jpg","src/test/resources/qiche.jpg",
		"src/test/resources/gou.jpg","src/test/resources/qiche.jpg"]
	model = alexnet(pretrained=True)#alexnet(pretrained=True)
	intermediate,output = vision.get_intermediate_output(images, model)
	# vision.check_labels(output)

	combos = []
	for tn in [9,12, 16,32]:
		for tb in [1,2,4]:
			for ti in [ 8, 16,4,32,2]:	
				for lp in [["N", "I", "B"], ["I", "N", "B"], ["B", "I", "N"], ["B", "N", "I"], ["I", "N", "B"], ["I", "B", "N"]]:
					for radix in [2,4,16]:
						for ws in [0.9,0.1, 0.5, 0.9]:
							for acts in [0.1, 0.5, 0.9]:
								combos.append([ti,tn,tb,lp, radix, ws, acts])								
	#from tqdm import tqdm
	for c in combos:#[combos[0]]):#[combos[0]]:
		ti, tn, tb, lp, radix, ws, acts = c
		print(c)
		#exit()
		#3. corresponding infer --> get traces
		config = {
        # CNN 相关参数
        "CNN_TI" : 2,
        "CNN_TN" : 2, 
        "CNN_TB" : 1, 

        "CNN_TNN" : 0,
        "CNN_TII" : 0,
        "CNN_TBB" : 0,

	"CNN_TX" : 2,
	"CNN_TY" : 2,
	"CNN_TXX": 0,
	"CNN_TYY": 0,

	"CNN_TKX": 3,
	"CNN_TKY": 3,

	"CNN_LOOP_ORDER": ["B", "N", "I", "X", "Y"]

	}
	
		config.update({
        # memory (by bits)
        "WEI_PREC": 8,#(todos) make editable
        "ACT_PREC": 8,
	
	"PE_PREC": 16,
	"ACC_PREC": 32,
        
        "DRAM_LEN": 512, #bits 
        
        "L2_LEN" : 512*8, #bits
        
        "L1_WEI_LEN" : config["CNN_TI"]*config["CNN_TN"]*config["CNN_TKX"]*config["CNN_TKY"]*8, #bits
        "L1_ACT_LEN" : config["CNN_TI"]*config["CNN_TB"]*config["CNN_TX"]*config["CNN_TY"]*8, #bits

		"multiplier2_radix": radix,
		
		"clock": 1,
		"cap_load":0.1,# 1.0, #0.1,
		"tech": "tsmc40",
	})

		sa = WinogradArch(config, model_name = model.__class__.__name__.lower(), 
		np_save = 1, 
		run_cpp = 1, 

		RUN_MODEL =1,
		RUN_GOLDEN=1,

		RUN_MAPPER=1,
		RUN_PE=0,
		RUN_WEI_BUFFERS=0,
		RUN_ACT_BUFFERS=0,
		RUN_ADDERS=0,#1,
		RUN_L1 = 0,
		RUN_L2 = 0,
	
		logs = "log_winograd",

		SIM_CYCLES = 1000, Randomize = True,
		EDAVerification =True,# False,#False,
		Wei_Sparse = ws, Act_Sparse = acts ) #0 more sparse, 1 less sparse
		sa.infer_network(intermediate)#, skips = [nn.Conv2D, nn.Linear])
		exit()
		
